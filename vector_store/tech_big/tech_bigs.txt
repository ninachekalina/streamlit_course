Тема: Введение в Большие данные
Динамика современного мира
Причины интереса к большим данным(Big Data) заключаются в безграничных объемах данных и наших технологических возможностях извлекать из них коммерческие инсайты.
Big Data (Большие данные) — это термин, используемый для описания больших объемов данных, которые слишком сложны для традиционных систем управления базами данных и аналитики. В зависимости от контекста и подхода, существует несколько различных определений и взглядов на Big Data:
Определение Больших данных
Традиционное определение:
Объем: Данные, которые настолько велики, что традиционные системы обработки данных (например, реляционные базы данных) не могут с ними справиться.
Типы данных: Включают как структурированные, так и неструктурированные данные, такие как текст, изображения, видео, сенсорные данные и т.д.
Особенности: Огромное количество данных, которое требует новых подходов к хранению, обработке и анализу (например, распределенные системы, Hadoop, NoSQL базы данных).
Определение по 3V (обычно используется в бизнес- и аналитической среде):
Volume (Объем): Количество данных. Чем больше данных, тем сложнее с ними работать.
Velocity (Скорость): Скорость поступления данных и необходимость обработки их в реальном времени или близко к реальному времени.
Variety (Разнообразие): Разнообразие типов данных, включая структурированные (например, данные в таблицах), полуструктурированные (например, XML, JSON) и неструктурированные (например, текст, видео, изображения).
 Определение по 5V:
Volume (Объем): Огромное количество данных.
Velocity (Скорость): Поток данных, который может поступать с высокой частотой.
Variety (Разнообразие): Разные типы данных.
Veracity (Достоверность): Качество данных, их точность и надежность.
Value (Ценность): Способность извлечь ценную информацию из больших объемов данных.
Углубление в V-аспекты
Скорость: Данные генерируются чрезвычайно быстро. Примеры включают потоковое видео и облачные технологии, обрабатывающие информацию в реальном времени.
Объем: Мы создаем примерно 2.5 квинтильона байт данных ежедневно.
Разнообразие: Данные поступают из различных источников: текст, звук, данные здоровья, устройства, подключенные к интернету вещей.
Точность: Примерно 80% данных считается неструктурированными, и необходимо разрабатывать методы для получения надежной и точной информации.
Тема: Введение в Большие данные. Определение Больших данных
Техническое определение:
Технологический подход: Большие данные — это данные, которые не могут быть эффективно обработаны традиционными средствами, такими как SQL-базы данных или серверы. Для их обработки используют распределенные вычисления, параллельную обработку и специализированные инструменты, например, Hadoop, Spark, и NoSQL базы данных.
 Интернет вещей (IoT) и Big Data:
В контексте Интернета вещей, большие данные возникают как результат непрерывного потока данных с различных сенсоров, устройств и машин, которые генерируют данные в реальном времени.
 Определение с акцентом на аналитические возможности:
Big Data как источник инсайтов: Это не просто большие объемы данных, но и возможность получать из них полезные инсайты с помощью аналитических инструментов, машинного обучения и искусственного интеллекта.
 Бизнес-определение:
Цель использования: Большие данные могут быть использованы для повышения конкурентоспособности, прогнозирования рыночных тенденций, улучшения обслуживания клиентов, оптимизации процессов и др.
 Социально-культурное определение:
Данные как новый ресурс: В современном мире Big Data рассматривается как новая «нефть», которая может быть использована для создания инновационных бизнес-моделей, социальных изменений и технологических прорывов.
Альтернативные термины:
Data Lake (Озеро данных): Хранилище, которое может содержать огромные объемы неструктурированных данных, которые могут быть использованы для аналитики.
Smart Data (Умные данные): Это данные, которые предварительно обработаны и отфильтрованы для извлечения ценной информации, в отличие от просто "больших данных".
Massive Data (Массивные данные): Это термин, который подчеркивает не только объем данных, но и их влияние на системы и бизнес.
Data Science (Наука о данных): Хотя это более широкая дисциплина, она тесно связана с анализом больших данных для извлечения информации и прогнозов.
Тема: Обзор Больших Данных
Введение в Большие данные
Сегодня многие из нас создают и используют Большие данные, даже не осознавая этого. Но как они влияют на бизнес и людей в России?
Примеры влияния Больших данных на потребительский опыт
Рекомендательные системы: Вы когда-нибудь покупали что-то на Ozon или Яндекс.Маркете? Обратите внимание, что эти платформы делают персонализированные рекомендации на основе ваших поисков и покупок. Компании, такие как Ozon и Яндекс, используют алгоритмы на основе Больших данных для предоставления персонализированных предложений, основываясь на предпочтениях и истории поведения клиентов.
Личные помощники: Личные ассистенты, такие как Алиса от Яндекса или Siri на устройствах Apple, используют Большие данные для предоставления ответов на множество вопросов пользователей. Например, Алиса может подсказывать, какой фильм посмотреть на основе ваших предпочтений.
Влияние Больших данных на бизнес
В 2021 году аналитическая компания IDC прогнозировала, что объем данных в мире будет расти, и это станет ключевым фактором в конкурентной борьбе, способствующим новым волнам роста производительности и инновациям.
Российские компании, такие как "Сбер" и "Тинькофф", активно используют Большие данные для улучшения качества обслуживания клиентов. Например, "Сбер" использует алгоритмы для анализа поведения клиентов и предсказания их потребностей, что позволяет максимально эффективно удовлетворять запросы.
Применение Больших данных в электронной коммерции
Российские компании в сфере электронной коммерции, такие как Wildberries, должны более эффективно использовать Большие данные для достижения конкурентных преимуществ. Это позволяет им повысить частоту покупок, размер покупок и разнообразие категорий товаров, покупаемых онлайн.
Интернет вещей (IoT): IoT обозначает резкий рост количества подключенных устройств. В России примеры IoT-устройств включают умные холодильники, системы домашней автоматизации и носимые устройства, такие как фитнес-браслеты и смарт-часы. Все эти устройства собирают  данные для обработки, что открывает новые возможности для анализа и использования Больших данных.
Какая из технологий используется для обработки больших данных?
Что из перечисленного является подходящим признаком к определению "больших данных"(Big Data)?
Что из следующих аспектов является частью определения Big Data по модели 5V? (Выберите все правильные ответы)
Какой термин описывает данные, которые должны быть предварительно обработаны и отфильтрованы для извлечения ценной информации?
Тема: Примеры и Источники Больших Данных
Введение в Большие данные
Объем данных: За последние два года создано больше данных, чем за всю предыдущую историю человечества.За последние два года было создано 90% всех мировых данных.
Прогнозы: В 2024 году для каждого человека на планете будет создаваться примерно 2.5 мегабайта информации каждую секунду.
Общий объем данных: Объем данных достиг 60 зеттабайт в 2023 году, по сравнению с 35 зеттабайтами в 2020 году.
Приближаемся к  зеттабайтам.
Бит: Единица данных, принимающая значения 0 или 1.
Байты: 8 битов = 1 байт.
Килобайты (КБ): 1024 байта = 1 КБ.
Мегабайты (МБ): 1024 КБ = 1 МБ.
Гигабайты (ГБ): 1024 МБ = 1 ГБ (используются в крупных видеофайлах и на DVD).
Терабайты (ТБ): 1024 ГБ = 1 ТБ (используются в современных жестких дисках).
Петабайты (ПБ): 1024 ТБ = 1 ПБ.
Эксабайты (ЭБ): 1024 ПБ = 1 ЭБ.
Прогнозы по Большим данным
Объем информации: По данным IDC, к концу 2023 года объем цифровой информации достиг 59 зеттабайт, прогнозируется рост до 175 зеттабайт к 2025 году.
Производство данных: Ожидается, что до 30% данных будут производиться машинами, большинство данных будет создаваться в развивающихся рынках.
Рост данных: Прогнозируется, что объем данных будет расти быстрее, чем доступные объемы хранения.
Влияние облачных технологий на Большие данные
Cloud Computing: Облачные технологии позволяют пользователям получать доступ к масштабируемым вычислительным и хранилищным ресурсам через интернет.
Расширение мощностей: Компании могут быстро увеличивать ресурсы для обработки больших объемов данных и запуска сложных математических моделей.
Основные источники Больших данных
Согласно исследованию IBM:
Данные, генерируемые людьми: Создаются через взаимодействие пользователей с цифровыми продуктами и услугами.
Данные, генерируемые машинами: Получаются от устройств, сенсоров и автоматизированных систем.
Данные, генерируемые бизнесом: Создаются организациями в процессе их деятельности.
Различные типы Больших данных
Большие данные могут быть трех типов:
Структурированные данные: Организованные и отформатированные данные, которые легко обрабатывать с помощью языков запросов, таких как SQL. Примеры включают реляционные базы данных и таблицы Excel.
Неструктурированные данные: Около 80% мировых данных составляют неструктурированные данные, которые обычно представлены в текстовом формате и не имеют заранее установленной модели.
Полуструктурированные данные: Эти данные могут иметь организованную структуру, но не следуют строгой модели. Примеры включают XML и JSON файлы.
Примеры больших объемов данных в России
Проект "Мир": Российская система глобального позиционирования (ГЛОНАСС) генерирует большие объемы данных, собирая информацию от спутников и наземных станций, включая данные о местоположении, скорости и состоянии транспортных средств. Это позволяет обеспечить навигацию и мониторинг в реальном времени.
Система "Умный город": В российских городах, таких как Москва, активно внедряются технологии "Умного города". Эта система использует данные с камер видеонаблюдения, датчиков окружающей среды и общественного транспорта для оптимизации городских служб и улучшения качества жизни горожан.
Московская биржа: Московская биржа активно использует технологии обработки Больших данных для анализа финансовых рынков. С помощью алгоритмов анализа и визуализации данных биржа может оценивать ликвидность, отслеживать тренды и управлять рисками. Системы, которые используются для обработки и анализа данных о торгах, предоставляют трейдерам и инвесторам актуальную информацию, что помогает им принимать более обоснованные решения.
Что из перечисленного является примером неструктурированных данных?
Какие данные генерируют машины? 
Какие технологии способствуют росту объема данных?
Какой прогноз по объему данных ожидается в 2025 году?
Тема: Связь Больших данных и Науки о данных
Основные аспекты Больших данных
Когда речь идет о больших данных, можно выделить несколько ключевых тем:
Интеграция:
Определение: Объединение различных частей в единую систему для упрощения обработки и анализа данных.
Примеры: Финансовые учреждения, такие как банки, должны интегрировать свои розничные, корпоративные и инвестиционные услуги.
Технологии: Использование платформы Hadoop (HDFS) для создания централизованного хранилища данных, что позволяет хранить и обрабатывать данные из различных источников.
Анализ:
Пример: Московская биржа (MOEX) использует аналитические платформы для анализа данных о торгах и котировках. Это позволяет отслеживать изменения в ценах и объемах торгов.
Ценность: Системы мониторинга анализируют поведение инвесторов, что помогает принимать обоснованные решения и повышает эффективность торговли.
Визуализация:
Значение: Представление данных в графическом виде для упрощения восприятия информации.
Пример: Отображение курсов акций на графиках или диаграммах, что помогает аналитикам легко анализировать динамику цен и объемы торгов.
Пользователи: Визуализация полезна как для специалистов по данным, так и для людей без технического образования.
Безопасность и управление:
Конфиденциальность данных: Важный аспект работы с большими данными. Бизнес и частные лица должны учитывать, как данные собираются, хранятся, используются и раскрываются.
Управление данными: Включает три ключевых аспекта:
Автоматизированная интеграция: Легкий доступ ко всем данным независимо от их места хранения.
Визуальный контент: Упрощенная категоризация, индексация и обнаружение данных.
Программное обеспечение и инструменты:
Основные приложения для работы с большими данными включают Hadoop, Apache Spark и MapReduce. Специалисты должны обладать навыками работы с этими инструментами для анализа больших объемов данных.
Наука о данных
Определение: Процесс очистки, добычи и анализа данных для извлечения ценной информации.
Объем данных: В науке о данных размер данных менее важен; можно работать как с малым, так и с большим объемом данных.
Процесс извлечения инсайтов: Основан на исследовательском анализе данных и моделировании, что позволяет информировать решения.
Ключевые навыки специалиста по данным
Специалист по данным должен обладать следующими навыками:
Определение проблемы: Четкое понимание бизнес-проблемы и целей проекта.
Сбор данных: Определение релевантных данных и проблем с конфиденциальностью.
Исследование данных: Обнаружение паттернов и трендов в данных.
Анализ данных: Разработка моделей, их тестирование и валидация.
Визуализация и коммуникация: Способность рассказывать историю с помощью данных.
Принятие решений: Умение делать выводы и принимать решения на основе анализа данных.
Какая из следующих технологий используется для создания централизованного хранилища данных, которое позволяет интегрировать данные из различных источников?
Что является основным назначением визуализации данных?
Какую роль играют аналитические платформы на примере Московской биржи?
Какой инструмент используется для работы с большими данными для обработки данных и выполнения параллельных вычислений?
Что является ключевым аспектом работы в области "Науки о данных"?
Что является основным навыком специалиста по данным?
Тема: Основные компоненты и экосистема Больших данных
Что такое экосистема Big Data?
Экосистема Big Data представляет собой совокупность технологий, инструментов и методов, которые позволяют эффективно собирать, хранить, обрабатывать, анализировать и визуализировать огромные объемы данных.
О чем же данный курс?
📌 Об источниках данных в Big Data
В уроке 2  разделили все источники данных в экосистеме Big Data  на структурированные, полуструктурированные и неструктурированные.
Структурированные данные - это организованные данные, которые хранятся в таблицах реляционных баз данных (SQL, PostgreSQL, MySQL).
Вспомним примеры структурированных данных :
 Финансовые транзакции (банковские операции, платежи).
Данные CRM-систем (клиенты, заказы, продажи).
 ERP-системы (инвентаризация, управление персоналом).
Полуструктурированные данные имеют определенную структуру, но не организованы в традиционные таблицы.
Вспомним примеры полуструктурированных данных:
 JSON, XML – широко используются в API и веб-службах.
 Лог-файлы серверов – журналы активности пользователей.
 E-commerce данные – списки товаров, метаданные о продуктах.
Неструктурированные данные - это неорганизованные данные, требующие предварительной обработки перед анализом.
Вспомним примеры неструктурированных данных:
✔️ Социальные сети (текст, изображения, видео).
✔️ IoT и сенсоры (потоковые данные от устройств).
✔️ Видео- и аудиофайлы (анализ изображений и речи).
Хранилища данных
Хранилища данных - хранение данных в Big Data требует масштабируемых решений, способных работать с огромными объемами информации.
Data Warehouse (DWH) - это централизованные хранилища, ориентированные на аналитические запросы.
Приведем примеры:
✔️ Google BigQuery – облачное хранилище с SQL-запросами.
✔️ Amazon Redshift – решение для обработки аналитических данных.
✔️ Snowflake – высокопроизводительное хранилище данных.
Плюсы:
✔️ Подходит для традиционной аналитики.
✔️ Высокая скорость запросов.
✔️ Оптимизировано для BI-инструментов.
Минусы:
Плохо подходит для неструктурированных данных.
 Дорогое хранение больших объемов информации.
Озеро данных (Data Lake) – это распределенное хранилище, в котором можно хранить сырые, неструктурированные и полуструктурированные данные.
Приведем примеры:
Hadoop HDFS – популярная файловая система для Big Data.
Amazon S3, Azure Data Lake – облачные решения для хранения.
Плюсы:
  Поддержка любых типов данных.
Гибкость и масштабируемость.
 Подходит для машинного обучения и AI.
Минусы:
Медленный доступ к данным без предварительной обработки.
Неоптимально для SQL-запросов.
NoSQL-хранилища - нереляционные базы данных, предназначенные для хранения данных в JSON, документов, графов и временных рядов. Альтернативы традиционным реляционным базам данных, предназначенные для гибкой работы с неструктурированными данными.
Приведем  некоторые примеры:
 MongoDB – документно-ориентированное хранилище.
 Cassandra – распределенная NoSQL-база для потоковых данных.
Redis – быстрый кэш для временных данных.
Плюсы:
 Гибкость хранения данных.
 Высокая скорость работы.
 Подходит для реального времени.
Минусы:
 Не всегда поддерживает сложные аналитические запросы.
 Обработка данных в Big Data
Обработка данных может быть пакетной (Batch Processing) и потоковой (Streaming Processing).
Пакетная обработка (Batch Processing) - данные обрабатываются партиями через регулярные интервалы времени.
Примеры инструментов:
Apache Hadoop –платформа с открытым исходным кодом для хранения и обработки больших данных (распределенная обработка данных).
Apache Spark – распределенная вычислительная платформа для обработки данных в реальном времени (высокоскоростной фреймворк для Big Data).
Применение:
Финансовая отчетность.
Анализ исторических данных.
Корпоративные аналитические системы.
Потоковая обработка (Streaming Processing) -данные обрабатываются в реальном времени сразу после поступления.
Примеры инструментов:
✔️ Apache Kafka – система обработки потоков событий.
✔️ Apache Flink – обработка потоковых данных с низкой задержкой.
Применение:
✔️ Мониторинг событий в реальном времени (биржи, соцсети).
✔️ Обработка данных IoT (умные устройства, датчики).
Аналитика и машинное обучение в Big Data
BI-аналитика - BI-инструменты помогают визуализировать и анализировать данные.
Примерами могут быть:
 Tableau– популярная BI-платформа.
Apache Superset – open-source инструмент для визуализации.
Применение:
✔️ Бизнес-аналитика.
✔️ Анализ KPI и метрик.
Машинное обучение (ML) в Big Data
 ML-модели применяются для прогнозирования, кластеризации и анализа данных.
Примеры инструментов:
✔️ TensorFlow, PyTorch – глубокое обучение.
✔️ Apache MLlib, H2O.ai – ML-библиотеки для больших данных.
Применение:
✔️ Рекомендательные системы.
✔️ Анализ поведения пользователей.
✔️  Прогнозирование спроса.
  Управление данными и оркестрация
Для автоматизации ETL/ELT-процессов используются:
✔️ Apache Airflow – управление рабочими процессами.
✔️ Kubernetes – оркестрация контейнеров.
Применение:
✔️  Автоматизация обработки данных.
✔️ Масштабирование вычислений.
В отчете "Global DataSphere Forecast", подготовленном компанией IDC в 2022 году, основные компоненты и экосистема Больших данных описаны следующим образом:
✔️ Методы анализа данных
Приведем подходы к извлечению ценной информации из больших объемов данных:
Машинное обучение: Использование алгоритмов для анализа данных и предсказания.
Глубокое обучение: Подход, основанный на нейронных сетях для сложных данных.
Обработка естественного языка (NLP): Анализ текстов для понимания человеческого языка.

A/B тестирование: Сравнение двух версий (A и B) для оценки эффективности изменений.
✔️ Технологии Больших данных
Какие инструменты и платформы, используются для хранения и обработки больших наборов данных? Посмотрим ниже на схему.
 ✔️ Визуализация данных
Процесс представления данных в графическом виде для упрощения анализа.
Примерами визуализации могут быть - создание диаграмм, графиков и интерактивных панелей для представления результатов анализа. Визуализация может помочь пользователям быстро понять сложные данные и принимать обоснованные решения.
✔️ Управление данными и безопасность
 Подходы к защите и управлению персональными и организационными данными.
Ключевыми аспектами являются:
Защита данных: соблюдение законодательства по защите данных.
Стратегии управления данными: разработка политик по сбору, хранению и использованию данных.
Ключевые моменты:
Технологии обработки больших данных помогают привести большие наборы структурированных и неструктурированных данных в формат для анализа и визуализации. Ценность из больших данных может быть извлечена только в том случае, если данные можно преобразовать в форматы, удобные для понимания.
📌 Экосистема Big Data охватывает сбор, хранение, обработку, анализ и визуализацию данных.
📌 Выбор инструментов зависит от масштаба, скорости и сложности обработки данных.
📌 Big Data-технологии позволяют автоматизировать процессы и внедрять AI-решения.
Комплексный подход к работе с Big Data помогает бизнесу принимать обоснованные решения, анализировать данные в реальном времени и строить эффективные аналитические системы.
Что из перечисленного относится к методам анализа данных?
Какие из следующих технологий работают с неструктурированными данными?
Выберите все подходящие ответы из списка
Что является ключевыми аспектами управления данными и безопасности?
Какая технология используется для обработки данных в реальном времени?
MySQL - это очень популярная система управления реляционными базами данных с открытым исходным кодом (СУБД).
Что такое MySQL?
MySQL - это система управления реляционными базами данных
-  имеет открытый исходный код
-  является бесплатной
- идеально подходит как для небольших, так и для больших приложений
 - очень быстрый, надежный, масштабируемый и простой в использовании
MySQL является кроссплатформенным
MySQL совместим со стандартом ANSI SQL
MySQL был впервые выпущен в 1995 году
Разрабатывается, распространяется и поддерживается корпорацией Oracle
Кто использует MySQL?
Огромные веб-сайты, такие как Facebook, Twitter, Airbnb, Booking.com , Uber, GitHub, YouTube и т.д.
Системы управления контентом, такие как WordPress, Drupal, Joomla!, Contao и т.д.
Очень большое количество веб-разработчиков по всему миру.
Что такое СУБД?
СУБД расшифровывается как Система управления реляционными базами данных.
СУБД - это программа, используемая для ведения реляционной базы данных.
СУБД является основой для всех современных систем баз данных, таких как:
MySQL,
Microsoft SQL Server,
Oracle
 Microsoft Access.
СУБД использует SQL-запросы для доступа к данным в базе данных.
Что такое таблица базы данных?
Таблица - это набор связанных записей данных, состоящий из столбцов и строк.
Столбец содержит конкретную информацию о каждой записи в таблице.
Запись (или строка) - это каждая отдельная запись, существующая в таблице.
Посмотрите на выборку из таблицы  "Клиенты":
Столбцы в таблице "Клиенты" выше:
Идентификатор клиента,
имя_пользователя,
имя_контакта,
Адрес,
Город,
почтовый индекс
страна.
Таблица содержит 5 записей (строк).
Что такое Реляционная база данных?
Реляционная база данных определяет отношения между базами данных в форме таблиц. Таблицы связаны друг с другом - на основе общих для каждой из них данных.
Посмотрите на следующие три таблицы "Клиенты", "Заказы" и "Грузоотправители" из базы данных Northwind:
Таблица клиентов
Связь между таблицей "Клиенты" и таблицей "Заказы" - это столбец CustomerID.
Таблица Заказов
Связь между таблицей "Заказы" и таблицей "Грузоотправители" - это столбец ShipperID.
Таблица грузоотправителей
Что такое SQL?
SQL - это стандартный язык для работы с реляционными базами данных.
SQL используется для вставки, поиска, обновления и удаления записей базы данных.
Как использовать SQL
Следующая инструкция SQL выбирает все записи в таблице "Клиенты":
SELECT * FROM Customers;
Имейте Это В Виду.
Ключевые слова SQL не чувствительны к регистру: select - это то же самое, что SELECT!
Точка с запятой после SQL-инструкций?
В некоторых системах баз данных требуется точка с запятой в конце каждой инструкции SQL.
Точка с запятой - это стандартный способ разделения каждой инструкции SQL в системах баз данных, которые позволяют выполнять более одной инструкции SQL при одном вызове сервера.
Некоторые из наиболее важных команд SQL
SELECT  - извлекает данные из базы данных
UPDATE- обновляет данные в базе данных
DELETE - удаляет данные из базы данных
INSERT INTO - вставляет новые данные в базу данных
CREATE DATABASE - создает новую базу данных
ALTER DATABASE - изменяет базу данных
CREATE TABLE  - создает новую таблицу
ALTER TABLE - изменяет таблицу
DROP TABLE - удаляет таблицу
CREATE INDEX- создает индекс (ключ поиска)
DROP INDEX - удаляет индекс
Инструкция MySQL CREATE DATABASE
Инструкция CREATE DATABASE используется для создания новой базы данных SQL.
Синтаксис:
CREATE DATABASE databasename;
Пример СОЗДАНИЯ БАЗЫ данных
Следующая инструкция SQL создает базу данных с именем "testDB".:
CREATE DATABASE testDB;
Заметка! 
После создания базы данных вы можете проверить ее в списке баз данных с помощью следующей команды SQL:SHOW DATABASES;
Инструкция MySQL CREATE TABLE.
Инструкция CREATE TABLE используется для создания новой таблицы в базе данных.
Синтаксис:
CREATE TABLE table_name (
    column1 datatype,
    column2 datatype,
    column3 datatype,
   ....
);
Параметры столбца задают имена столбцов таблицы.
Параметр datatype указывает тип данных, которые может содержать столбец (например, varchar, целое число, дата и т.д.).
Пример СОЗДАНИЯ ТАБЛИЦЫ MySQL
В следующем примере создается таблица с именем "Persons", содержащая пять столбцов: PersonID, фамилия, отчество, Адрес и город:
CREATE TABLE Persons (
    PersonID int,
    LastName varchar(255),
    FirstName varchar(255),
    Address varchar(255),
    City varchar(255)
);
Столбец PersonID имеет тип int и будет содержать целое число.
Столбцы Фамилия, Имя, Адрес и Город имеют тип varchar и будут содержать символы, а максимальная длина этих полей составляет 255 символов.
Пояснение: Пустая таблица "Persons" теперь может быть заполнена данными с помощью инструкции SQL INSERT INTO.
Оператор MySQL SELECT.
Оператор SELECT используется для выбора данных из базы данных.
Возвращаемые данные сохраняются в таблице результатов, называемой результирующим набором.
Синтаксис Select:
SELECT column1, column2, ...
FROM table_name;
Здесь column1, column2, ... - это имена полей таблицы, из которой вы хотите выбрать данные. Если вы хотите выбрать все поля, доступные в таблице, используйте следующий синтаксис:
SELECT * FROM table_name;
Пример ВЫБОРА столбцов:
Следующая инструкция SQL выбирает столбцы  "Имя_пользователя", "Город" и "Страна" из таблицы "Клиенты": 
SELECT CustomerName, City, Country FROM Customers;
Следующая инструкция SQL выбирает ВСЕ столбцы из таблицы "Клиенты":
SELECT * FROM Customers;
Оператор MySQL SELECT DISTINCT.
Оператор SELECT DISTINCT используется для возврата только различных значений.
Внутри таблицы столбец часто содержит много повторяющихся значений; и иногда вы хотите перечислить только разные (отличные) значения.
  Синтаксис SELECT DISTINCT:
SELECT DISTINCT column1, column2, ...
FROM table_name;
Пример SELECT без DISTINCT:
Следующая инструкция SQL выбирает все (включая дубликаты) значения из столбца "Страна" в таблице "Клиенты"(см. 1.1 шаг 2):
SELECT Country FROM Customers;
Пример оператора SELECT DISTINCT :
Следующая инструкция SQL выбирает только УНИКАЛЬНЫЕ значения из столбца "Страна" в таблице "Клиенты":
SELECT DISTINCT Country FROM Customers;
Следующая инструкция SQL подсчитывает и возвращает количество разных (отличных) стран в таблице "Клиенты":
SELECT COUNT(DISTINCT Country) FROM Customers;
Предложение WHERE используется для фильтрации записей.
Он используется для извлечения только тех записей, которые удовлетворяют указанному условию.
Cинтаксис WHERE 
SELECT column1, column2, ...
FROM table_name
WHERE condition;
Заметка! Предложение WHERE используется не только в операторах SELECT, оно также используется в UPDATE, DELETE и т.д.!

Ниже приведена выборка из таблицы "Клиенты" в базе данных :

Пример предложения WHERE
Следующая инструкция SQL выбирает всех клиентов из "Мексики":
SELECT * FROM Customers
WHERE Country = 'Mexico';
SQL требует одинарных кавычек вокруг текстовых значений (большинство систем баз данных также допускают двойные кавычки).
Однако числовые поля не следует заключать в кавычки:
SELECT * FROM Customers
WHERE CustomerID = 1;
Операторы в предложении WHERE
В предложении WHERE могут быть использованы следующие операторы:
Оператор	Описание
=	Равно
>	Больше чем
<	Меньше чем
>=	Больше чем или равно
<=	Меньше чем или равно
<>	Не равно
BETWEEN	Между определенным диапазоном
LIKE	Поиск шаблона
IN	Чтобы указать несколько возможных значений для столбца
Операторы MySQL И, ИЛИ и НЕ
Предложение WHERE может быть объединено с операторами AND, OR и NOT.
Операторы И и ИЛИ используются для фильтрации записей на основе более чем одного условия:
Оператор AND отображает запись, если все условия, разделенные символом И, ВЕРНЫ.
Оператор OR отображает запись, если какое-либо из условий, разделенных символом ИЛИ, является ИСТИННЫМ.
Оператор NOT отображает запись, если условие  НЕВЕРНО. 
Синтаксис AND:
SELECT column1, column2, ...
FROM table_name
WHERE condition1 AND condition2 AND condition3 ...;
Синтаксис OR:
SELECT column1, column2, ...
FROM table_name
WHERE condition1 OR condition2 OR condition3 ...;
 Синтаксис NOT 
SELECT column1, column2, ...
FROM table_name
WHERE NOT condition;
Пример AND
Следующая инструкция SQL выбирает все поля из "Клиентов", где страна - "Германия", А город - "Берлин".:
SELECT * FROM Customers
WHERE Country = 'Germany' AND City = 'Berlin';
Результат:
Количество записей: 1
Пример OR
Следующая инструкция SQL выбирает все поля из "Клиенты", где город - "Берлин" ИЛИ "Штутгарт".:
SELECT * FROM Customers
WHERE City = 'Berlin' OR City = 'Stuttgart';
Количество записей: 2
Следующая инструкция SQL выбирает все поля из "Customers", где страной является "Германия" ИЛИ "Испания".:
SELECT * FROM Customers
WHERE Country = 'Germany' OR Country = 'Spain';
Количество записей: 16
Пример NOT 
Следующая инструкция SQL выбирает все поля из "Customers", где страна НЕ является "Germany".:
SELECT * FROM Customers
WHERE NOT Country = 'Germany';
Сочетание операторов И, ИЛИ и НЕ
Вы также можете комбинировать операторы И, ИЛИ и НЕ.
Следующая инструкция SQL выбирает все поля из "Клиенты", где страна - "Германия",  а город должен быть "Берлин" ИЛИ "Штутгарт" (используйте круглые скобки для формирования сложных выражений):
SELECT * FROM Customers
WHERE Country = 'Germany' AND (City = 'Berlin' OR City = 'Stuttgart');
Следующий оператор SQL выбирает все поля из "Customers", где страна НЕ ЯВЛЯЕТСЯ "Germany" и НЕ "USA".:
SELECT * FROM Customers
WHERE NOT Country = 'Germany' AND NOT Country = 'USA';
Ключевое слово MySQL ORDER BY
Ключевое слово ORDER BY используется для сортировки результирующего набора в порядке возрастания или убывания.
Ключевое слово ORDER BY по умолчанию сортирует записи в порядке возрастания. Чтобы отсортировать записи в порядке убывания, используйте ключевое слово DESC.
Синтаксис ORDER BY :
SELECT column1, column2, ...
FROM table_name
ORDER BY column1, column2, ... ASC|DESC;
Ниже приведена выборка из таблицы "Клиенты" в базе данных:
Пример ORDER BY
Следующая инструкция SQL выбирает всех клиентов из таблицы "Клиенты", отсортированных по столбцу "Страна":
SELECT * FROM Customers
ORDER BY Country;
Пример ORDER BY DESC 
Следующая инструкция SQL выбирает всех клиентов из таблицы "Клиенты", отсортированных по убыванию по столбцу "Страна":
SELECT * FROM Customers
ORDER BY Country DESC;
Пример ORDER BY по нескольким столбцам:
Следующая инструкция SQL выбирает всех клиентов из таблицы "Customers", отсортированных по столбцу "Country" и "CustomerName". Это означает, что он сортирует по стране, но если некоторые строки имеют одну и ту же страну, он сортирует их по имени клиента:
SELECT * FROM Customers
ORDER BY Country, CustomerName;
Пример: 
Следующая инструкция SQL выбирает всех клиентов из таблицы "Клиенты", отсортированных по возрастанию по "Стране" и по убыванию по столбцу "Имя_пользователя":
SELECT * FROM Customers
ORDER BY Country ASC, CustomerName DESC;
Предложение JOIN используется для объединения строк из двух или более таблиц на основе связанного столбца между ними.
Давайте посмотрим на выборку из таблицы "Заказы":
Затем посмотрите на выборку из таблицы "Клиенты":
Обратите внимание, что столбец "Идентификатор клиента" в таблице "Заказы" ссылается на "Идентификатор клиента" в таблице "Клиенты". Связь между двумя приведенными выше таблицами - это столбец "CustomerID".
Затем мы можем создать следующую инструкцию SQL (содержащую ВНУТРЕННЕЕ соединение), которая выбирает записи, имеющие совпадающие значения в обеих таблицах:
SELECT Orders.OrderID, Customers.CustomerName, Orders.OrderDate
FROM Orders
INNER JOIN Customers ON Orders.CustomerID=Customers.CustomerID;
Поддерживаемые типы соединений в MySQL
INNER JOIN: ВНУТРЕННЕЕ СОЕДИНЕНИЕ: возвращает записи, которые имеют совпадающие значения в обеих таблицах
LEFT JOIN: СОЕДИНЕНИЕ ЛЕВОЕ: возвращает все записи из левой таблицы и сопоставленные записи из правой таблицы
RIGHT JOIN: ПРАВОЕ СОЕДИНЕНИЕ: возвращает все записи из правой таблицы и соответствующие записи из левой таблицы
CROSS JOIN: ПЕРЕКРЕСТНОЕ СОЕДИНЕНИЕ: возвращает все записи из обеих таблиц
Тема: Введение в NoSQL
познакомимся с определением NoSQL, его технологией, историей, причинами использования
 Определение NoSQL
Что такое NoSQL?
NoSQL — это термин, который был введен на мероприятии, посвященном новым открытым распределенным базам данных. Он расшифровывается как "not only SQL" (не только SQL), а не "no SQL" (нет SQL). Этот термин описывает семейство баз данных, которые обладают различными стилями и технологиями, но являются нереляционными.
 Существуют четыре основных типа NoSQL баз данных:
Ключ-значение (Key-Value)
Документированные (Document)
Столбцовые (Column-based)
Графовые (Graph)
Пример: NoSQL базы данных могут хранить данные не только в табличном формате, как это делает SQL, но и в форме документов, ключ-значений или графов.
Технология NoSQL
Новые методы хранения
NoSQL базы данных предлагают новые методы хранения и запроса данных, которые решают многие проблемы современных приложений. Они отличаются от реляционных баз данных тем, что не требуют фиксированных схем, что делает их подходящими для динамичных и изменяющихся условий использования.
Пример: В приложении для управления проектами данные о задачах и пользователях могут часто изменяться, что делает использование фиксированной схемы нецелесообразным.
 Масштабируемость
NoSQL базы данных позволяют легко горизонтально масштабироваться, что означает возможность добавления новых ресурсов по мере роста спроса.
Пример: В случае увеличения числа пользователей веб-приложения можно добавить новые серверы, на которых будут храниться части данных, чтобы избежать перегрузки.
Отказоустойчивость
NoSQL базы данных являются распределенными системами, которые обеспечивают встроенную отказоустойчивость и доступность.
Пример: Если один из серверов в кластере выходит из строя, данные все еще доступны на других серверах, что позволяет минимизировать время простоя.
 История NoSQL
 Этапы развития
Между 1970 и 2000 годами рынок был в основном занят реляционными базами данных, такими как Oracle, Microsoft SQL Server, MySQL.
С ростом интернет-приложений в конце 90-х и начале 2000-х годов возникла потребность в новых масштабируемых технологиях. Крупные технологические компании, такие как IBM, Google и Meta, разработали инновационные технологии и открыли свои разработки.
В конце 2000-х годов на сцену вышли новые базы данных из сообществ открытого программного обеспечения, такие как MongoDB, Cassandra, CouchDB и другие.
 Причины использования NoSQL
Гибкая модель данных: Легкость хранения неструктурированных и полуструктурированных данных.

Пример: Веб-приложения могут хранить данные о пользователях, их предпочтениях и действиях в различных форматах.

Масштабируемость: Наличие встроенных возможностей горизонтального и вертикального масштабирования.

Пример: Размещение базы данных на нескольких серверах позволяет легко расширять систему по мере роста нагрузки.

Повышенная продуктивность: Разработчики могут работать быстрее с данными, которые соответствуют потребностям приложений.

Пример: Использование NoSQL позволяет командам быстрее разрабатывать и тестировать новые функции продукта.
Высокая доступность: NoSQL базы данных работают в распределенных средах и обеспечивают отказоустойчивость.
Пример: Социальные сети используют распределенную архитектуру, чтобы поддерживать доступность данных даже в случае отказа отдельных узлов.
В результате, NoSQL базы данных являются мощным инструментом для решения современных задач в области хранения и обработки данных, предлагая гибкость и масштабируемость, необходимые для работы с растущими объемами информации.
Тема: Документные NoSQL базы данных. 
 Обзор
Документные базы данных, также известные как базы данных, ориентированные на документы, хранят данные в формате документа, обычно это JSON или BSON (бинарный JSON). В таких базах данных каждый документ содержит пары ключ-значение или ключ-документ. Они не имеют фиксированной схемы, что обеспечивает гибкость в структуре данных внутри коллекции.
Пример: В документной базе данных информация о пользователе может храниться в виде JSON-документа, содержащего имя, возраст и адрес.
 Архитектура
Гибкая схема
Каждый документ в документной базе данных может иметь уникальную структуру, что позволяет не требовать от документов одинаковой информации или формата. Это обеспечивает высокую гибкость при добавлении новых атрибутов.
Пример: Документы о пользователях могут содержать разные наборы данных — у одного пользователя могут быть указаны телефон и адрес, а у другого — только адрес.
 Индексация и запросы
Документные базы поддерживают возможность индексирования и выполнения различных запросов, включая  и аналитические запросы с использованием таких подходов, как MapReduce.
 Эффективные операции CRUD: Они хорошо подходят для приложений с интенсивным чтением и записью, так как позволяют извлекать целые документы.
Пример: Если мы хотим найти всех пользователей, зарегистрированных в определенный период, индексы помогут быстро извлечь необходимые данные.
Масштабируемость
Документные базы данных обеспечивают горизонтальную масштабируемость и позволяют шардирование — распределение данных по множеству узлов на основе уникального ключа в документе.
Пример: Если у вас много пользователей, вы можете распределить их данные по нескольким серверам, чтобы улучшить производительность.
 Атомарные транзакции
Гарантируется атомарность операций только для отдельных документов. Это значит, что изменения в одном документе будут выполнены полностью или не будут выполнены вовсе.
Пример: Если вы обновляете информацию о пользователе, изменения будут применены только в случае успешного выполнения.
Применение
Логирование событий
Каждое событие может быть представлено новым документом, содержащим всю информацию о произошедшем.
Пример: Логирование покупок в интернет-магазине, где каждый покупатель и его товар записываются в отдельные документы.
 Онлайн-блоггинг

Документы позволяют удобно хранить данные о пользователях, постах и комментариях.

Пример: Каждый пост может представлять отдельный документ с информацией о заголовке, содержимом и времени создания.

Операционные данные для веб- и мобильных приложений

Документные базы данных разработаны для работы в условиях интернет-среды, поддерживая технологии, такие как JSON и RESTful API.

Пример: Мобильное приложение может использовать документную базу для хранения данных о пользователе и его действиях.

Ограничения
 Требования к ACID транзакциям
Документные базы могут подходить для сценариев с требованиями ACID, но для приложений которые требуют строгих ACID  - предпочтительнее использовать реляционные СУБД.

 Нормализованная структура данных
Если ваши данные требуют нормализованной или табличной структуры, реляционные базы данных могут быть более подходящим выбором.

 Распространенность
Документные NoSQL базы данных являются одной из наиболее распространенных категорий NoSQL.

Примеры популярных реализаций включают:

IBM Cloudant
MongoDB
Apache CouchDB
Terrastore
OrientDB
Couchbase
RavenDB
Документные NoSQL базы данных используют модели документирования для организации данных. Каждый документ предлагает гибкую структуру, и основные случаи использования документных баз включают логирование событий, онлайн-блоггинг и управление операционными данными для веб- и мобильных приложений.
Тема: Обзор Key-Value NoSQL баз данных
Рассмотрим архитектуру этих баз данных, типовые сценарии их использования, ограничения и популярные реализации.
 Архитектура Key-Value баз данных
 Структура данных
В Key-Value базах данных информация хранится в виде уникального ключа и связанного с ним значения (или блоба). Эта архитектура представлена в виде хеш-таблицы.
Пример: Предположим, у нас есть веб-приложение, где мы храним информацию о пользователях. Ключом может быть идентификатор пользователя, а значением — объект, содержащий его имя и электронную почту. Например:

Ключ: user123
Значение: { "name": "Иван", "email": "ivan@example.com" }
 Простота
Key-Value базы данных являются наименее сложными с архитектурной точки зрения. Они идеально подходят для выполнения базовых операций создания, чтения, обновления и удаления (CRUD).

Пример: Для добавления нового пользователя можно просто создать пару «ключ-значение». Для извлечения данных достаточно использовать ключ.

 Масштабируемость
Эти базы данных хорошо масштабируются и легко шардируются (разделяются) по множеству узлов. Каждая шард содержит диапазон ключей и связанных с ними значений.

Пример: Если количество пользователей возрастает, данные могут быть распределены по нескольким серверам, где каждый сервер будет хранить определённый диапазон ключей.

 Применение
 Типичные сценарии использования
Key-Value базы данных являются отличным выбором в ситуациях, когда требуется быстрая обработка операций CRUD без сложных запросов и взаимосвязей между данными.

Примеры использования:

Хранение информации о сессиях веб-приложений:

Каждую сессию можно хранить как пару «ключ-значение». Ключом будет идентификатор сессии, а значением — информация о пользователе и его действиях.
Хранение пользовательских профилей и предпочтений:

Каждый пользователь может иметь свой уникальный ключ, соответствующий его профилю с личными данными и предпочтениями.
Сохранение данных о корзине покупок в онлайн-магазинах:

Информация о содержимом корзины пользователя может храниться как пара «ключ-значение», где ключ — идентификатор пользователя, а значение — список товаров.
 Ограничения
Неэффективность при сложных запросах

Key-Value базы недостаточно эффективны для случаев, когда данные имеют взаимосвязи, например, в социальных сетях или в системах рекомендаций. Здесь лучше использовать другие виды баз данных.

Отсутствие поддержки ACID

Если требуется высокая степень консистентности для транзакций, связанных с несколькими ключами, стоит рассмотреть другие базы данных, поскольку Key-Value базы не обеспечивают строгую поддержку ACID-транзакций.

Запросы по значению

Если предполагается необходимость выполнения запросов, основанных на значениях (например, по атрибутам), лучше обратить внимание на документные NoSQL базы данных, которые имеют более развитые механизмы поиска.

 Популярные реализации
Некоторые из популярных реализаций Key-Value NoSQL баз данных включают:

Amazon DynamoDB
Oracle NoSQL Database
Redis
Aerospike
React KV
MemcacheDB
Project Voldemort
Key-Value NoSQL базы данных представляют собой наиболее простой тип. Данные в них хранятся в виде пар «ключ-значение». Основные сценарии их использования включают создание приложений, где необходима быстрая обработка операций CRUD, таких как хранение данных о сессиях, пользовательских профилях и данных коризны покупок в онлайн-магазинах.
Тема: Введение в столбцовые NoSQL базы данных
Архитектура столбцовых баз данных
Происхождение и структура
Столбцовые базы данных были вдохновлены архитектурой Google под названием Bigtable и часто именуются клонами Bigtable, столбцовыми или широкими столбцовыми базами данных. В этих базах данных основное внимание уделяется колонкам и группам колонок в процессе хранения и доступа к данным.
Пример: Данные могут организовываться в столбцовые семейства, где каждое семейство содержит несколько строк. Каждая строка имеет уникальный ключ, который связывает её с одной или несколькими колонками. Это позволяет создавать гибкие структуры.

Гибкость структур
Строки в одном семействе не обязаны содержать одинаковые колонки. Это означает, что вы можете добавлять или изменять колонки для разных строк без необходимости изменять всю структуру базы данных.

Преимущества и недостатки
Преимущества
Эффективная работа с разреженными данными: Столбцовые базы данных отлично подходят для хранения больших объемов разреженных данных, так как могут лучше сжимать информацию и экономить место.
Горизонтальная масштабируемость: Эти базы легко масштабируются и позволяют развертывание на кластерах узлов. Это означает, что вы можете добавлять больше серверов для обработки большего объема данных.

Удобство ведения журналов событий: Каждое приложение может записывать данные в свои собственные столбцы, что удобно для мониторинга и аудита.
 Недостатки
Ограничения ACID-транзакций: Столбцовые базы не поддерживают традиционные ACID-транзакции, поскольку атомарные операции действуют только на уровне строк. Это может стать проблемой для  приложений, требующих гарантированной целостности данных.
Сложность в изменении паттернов запросов: При изменении требований к базе данных могут понадобиться значительные изменения в её дизайне, что может замедлить процесс разработки и усложнить поддержку.
Применение столбцовых баз данных
 Хранение и анализ данных
Столбцовые базы данных часто используются в хранилищах данных, где записи хранятся по столбцам. Это позволяет быстрее получать доступ к данным и улучшает аналитические возможности организации.

Пример: В финансовых институтах данные о транзакциях могут храниться в виде столбцов, что позволяет быстро анализировать объемы операций.
 Аналитика
Эти базы данных отлично подходят для финансового анализа и OLAP (онлайн аналитической обработки), где анализируется подмножество больших объемов данных.

Пример: аналитики могут использовать столбцовые базы для создания отчетов о продажах по месяцам, при этом обрабатывая только те данные, которые необходимо проанализировать.
 IoT (Интернет вещей)
С увеличением числа IoT-устройств столбцовые базы данных обеспечивают эффективное хранение и доступ к данным в режиме реального времени, что критично для приложений, таких как мониторинг транспортных средств.

Пример: Данные о местоположении и скорости грузовиков можно сохранять в столбцовой базе, что позволяет быстро проводить анализ производительности водителей.

 Примеры реализаций
Некоторые из популярных реализаций столбцовых NoSQL баз данных включают:
Cassandra
HBase
Hypertable
Accumulo
Столбцовые NoSQL базы данных основаны на архитектуре Google Bigtable и хранят данные в колонках или группах колонок. Они находят применение в таких областях, как хранилища данных, бизнес-аналитика и ведение журналов событий IoT. Если вас интересуют масштабируемые решения для работы с большими объемами данных, столбцовые базы данных могут стать отличным выбором.
Тема: Введение в графовые NoSQL базы данных
Основные характеристики графовых баз данных
Структура данных
Графовые базы данных хранят информацию в виде сущностей (узлов) и отношений (ребер). Это делает их особенно эффективными для работы с данными, у которых имеются взаимосвязи, что отлично подходит для графоподобных структур.
Пример: Рассмотрим социальную сеть. В ней пользователи являются узлами, а отношения дружбы между ними — ребрами. Это позволяет быстро рекомендовать новые знакомства на основе существующих связей.
Производительность
Обход всех взаимосвязей в графовых базах данных происходит быстро и эффективно. Однако стоит отметить, что графовые базы не так хорошо масштабируются по горизонтали. Шардирование данных не рекомендуется, поскольку это может усложнить процесс обхода графа и ухудшить производительность.

Пример: Если у вас есть массив узлов, разбитый на несколько серверов, выполнять запросы о том, кто дружит с кем, может быть затруднительно из-за необходимости взаимодействия между серверами. Например, для нахождения общего друга вам может потребоваться сложный запрос через несколько серверов.

Соответствие ACID
Графовые базы данных обеспечивают соответствие ACID-транзакциям, что гарантирует целостность данных и предотвращает появление "висячих" связей между узлами.

Пример: Если пользователь удаляет своего друга в социальной сети, система гарантирует, что это изменение будет атомарным и все связанные отношения будут обновлены корректно. Таким образом, удаление связи между двумя узлами также приведет к автоматическому обновлению всех связанных данных.

Примеры использования графовых баз данных
 Социальные сети
Графовые базы данных отлично подходят для социальных сетей, позволяя быстро находить друзей, их друзей и другие взаимосвязи. Это позволяет пользователям легко получать рекомендации по новым знакомствам.

Пример: ВКонтакте использует графовые базы данных для анализа дружеских отношений и предоставления рекомендаций по добавлению в друзья.

 Навигационные и картографические приложения
Графы позволяют удобно моделировать данные для нахождения ближайших мест или построения кратчайших маршрутов. Например, сервисы карт могут использовать графовые базы для нахождения наиболее оптимальных путей между двумя точками.

Пример:  Яндекс.Карты использует графовые базы данных для расчета маршрутов и определения оптимальных путей, учитывая расстояния и время.

Рекомендательные системы
Графы эффективно используют взаимосвязи между продуктами для предложения пользователям дополнительных опций. Например, когда вы смотрите товар в интернет-магазине, система может предложить аналогичные продукты на основе ранее купленных или просмотренных.

Пример: OZON использует графовые базы данных для рекомендаций, предлагая товары, основываясь на том, что покупали другие клиенты с похожими интересами.

 Обнаружение мошенничества
Графовые базы данных могут быть использованы для выявления подозрительных операций, связанных с финансовыми транзакциями.

Пример: Банк может использовать графовые базы для отслеживания трансакций между клиентами. Если две транзакции идут от одного клиента к нескольким пользователям за короткий промежуток времени, это может вызвать сигнал тревоги о возможном мошенничестве.
Недостатки и ограничения
Горизонтальное масштабирование
Графовые базы данных не подходят для приложений, которые требуют высокой горизонтальной масштабируемости, так как они могут быстро достичь своих пределов при увеличении объема данных.

Обновление узлов
Обновление всех или некоторых узлов может быть трудоемким и сложным процессом. Это требует дополнительных усилий, чтобы обеспечить целостность данных.
Популярные реализации графовых NoSQL баз данных
Вот некоторые известные графовые NoSQL базы данных:
Neo4j
OrientDB
ArangoDB
Amazon Neptune (часть Amazon Web Services)
Apache Giraph
JanusGraph
Графовые базы данных эффективно хранят информацию в узлах и связях. Они особенно полезны для работы с высокозависимыми и связанными данными, такими как социальные сети и рекомендательные системы. Мы также рассмотрели, как графовые базы могут использоваться в различных приложениях, включая обнаружение мошенничества и навигацию. Однако важно помнить об их ограничениях по горизонтальному масштабированию и сложности обновления узлов
Тема:  Введение в Hadoop
1. Что такое Hadoop?
Hadoop — это фреймворк с открытым исходным кодом, который используется для обработки огромных объемов данных.

Особенности Hadoop:

Обеспечивает обработку данных в распределенных файловых системах.
Позволяет запускать приложения на кластерах компьютеров.
Не является базой данных, а представляет собой экосистему для параллельной обработки данных.
Оптимизирован для работы с большими объемами структурированных, полуструктурированных и неструктурированных данных, используя недорогое оборудование.
Пример использования Hadoop
Представьте, что ваш бизнес развивается, и объемы данных растут:

Сначала вы обрабатываете гигабайты данных.
Затем данные увеличиваются до терабайтов и экзабайтов.
Вы начинаете работать с неструктурированными данными, такими как данные из социальных сетей, сенсоров, изображений и видео.
Решением для обработки таких данных может стать Hadoop, который справляется с большими объемами информации и позволяет анализировать их для принятия важных бизнес-решений.

История Hadoop
1999: Создана некоммерческая организация Apache Software Foundation.
2002: Появление поисковой системы Nutch, разработанной Дугом Каттингом и Майком Кафареллой.
2006: Проект Nutch был разделен на веб-сканер и систему распределенной обработки данных. Последняя получила название Hadoop.
2008: Yahoo передал Hadoop в Apache Software Foundation.
 Основные компоненты Hadoop
Hadoop состоит из нескольких ключевых модулей:

Hadoop Common: Набор библиотек и утилит, поддерживающий остальные модули Hadoop.
HDFS (Hadoop Distributed File System): Хранилище для больших наборов данных, работающее на обычном недорогом оборудовании.
MapReduce: Модуль для обработки данных, который делит большие объемы информации на небольшие части и обрабатывает их одновременно.
YARN (Yet Another Resource Negotiator): Компонент, управляющий ресурсами, такими как оперативная память и процессоры, для различных видов обработки данных (пакетная, потоковая и др.).
 Недостатки Hadoop
Несмотря на свои преимущества, Hadoop имеет ограничения:
Не подходит для обработки транзакций из-за отсутствия произвольного доступа.
Плохо справляется с задачами, которые нельзя выполнять параллельно или зависят от последовательной обработки данных.
Неэффективен для обработки небольших файлов.
Не подходит для задач с низкой задержкой (low latency).
Неэффективен для интенсивных вычислений с небольшим объемом данных.
 Решение недостатков
Для устранения недостатков Hadoop были созданы дополнительные инструменты:
Hive: Позволяет выполнять запросы, похожие на SQL, с поддержкой статистических функций.
Pig: Упрощает многозапросный подход, сокращая количество сканирований данных.
Ключевые моменты:
Hadoop — это фреймворк с открытым исходным кодом для обработки больших данных.
Основные компоненты: HDFS, MapReduce, YARN, Hadoop Common.
Hadoop имеет свои недостатки, но их можно частично решить с помощью дополнительных инструментов, таких как Hive и Pig.
Тема: Введение в MapReduce
 Что такое MapReduce?
MapReduce — это программная модель, которая обеспечивает масштабируемость на сотни или тысячи серверов в кластере Hadoop.
Ключевые особенности:
MapReduce — это метод обработки данных и модель программирования для распределенных вычислений.
Основан на языке Java, но может быть реализован на C++, Python, Ruby, R и других языках.
Подразумевает работу множества компонентов, расположенных на разных компьютерах, которые взаимодействуют друг с другом как единая система для конечного пользователя.
 Основные компоненты MapReduce
MapReduce состоит из двух главных задач:
Map — обработка входных данных, извлечение информации и создание пар "ключ-значение".
Reduce — агрегация данных, полученных на этапе Map, для формирования окончательного результата.
 Как работает MapReduce?
Шаг 1: Map
Входные данные поступают из файла, сохраненного в распределенной файловой системе Hadoop (HDFS).
Пример задачи: подсчет частоты появления имен в текстовом файле.
Данные делятся на несколько файлов, каждый из которых обрабатывается отдельно.
Выходной результат этапа Map — пары "ключ-значение".
Шаг 2: Reducer
На этапе Reduce данные сортируются и группируются по ключу (шаг "shuffling").
Значения с одинаковыми ключами агрегируются.
Итоговый результат сохраняется в HDFS.
Анализ активности в социальных сетях:
На платформах, MapReduce используется для анализа пользовательской активности:
Map: Данные собираются из профилей пользователей, таких как лайки, комментарии, подписки, и преобразуются в пары "пользователь-действие". Например:
Пользователь1: Лайк Пользователь2: Комментарий Пользователь1: Лайк 
Reduce: Эти пары агрегируются для подсчета частоты каждого действия. Итоговые результаты:
Пользователь1: 2 Лайка Пользователь2: 1 Комментарий
Рекомендательные системы (Netflix):
Map: Список фильмов, которые пользователь уже посмотрел, преобразуется в пары "фильм-жанр". Например:
Пользователь1: Драма Пользователь1: Комедия Пользователь2: Экшн

Reduce: На основании общего количества предпочтений пользователей система предлагает новые фильмы в тех же жанрах. Итог:
Пользователь1: Рекомендация (Драма, Комедия) Пользователь2: Рекомендация (Экшн)

Банковский анализ транзакций:
Финансовые учреждения используют MapReduce для выявления подозрительных транзакций:

Map: Каждая транзакция проверяется на соответствие установленным критериям (например, сумма транзакции или время). Преобразуется в пары "транзакция-критерий". Например:
Транзакция1: Высокая сумма Транзакция2: Ночная активность

Reduce: Агрегируются транзакции, соответствующие подозрительным критериям, для последующего анализа. Итог:
Транзакции для проверки: Транзакция1, Транзакция2

Рекламный анализ (Google Ads):

Map: Данные о взаимодействии пользователей с рекламой (просмотры, клики, время на странице) собираются и преобразуются в пары "реклама-действие". Например:
Реклама1: Клик Реклама2: Просмотр Реклама1: Клик

Reduce: Подсчитывается количество взаимодействий с каждой рекламой. Итоговые результаты позволяют оптимизировать кампанию:
Реклама1: 2 Клика Реклама2: 1 Просмотр

Обработка данных с датчиков в промышленности:

Map: С каждого датчика собираются данные о работе оборудования (температура, вибрации, мощность) и преобразуются в пары "датчик-параметр". Например:
Датчик1: Температура 80°C Датчик2: Вибрация 0.05g

Reduce: Агрегируются параметры для каждого датчика и выявляются отклонения. Итог:
Датчик1: Температура за пределами нормы

Эти примеры показывают универсальность и широкую применимость MapReduce для анализа больших объемов данных, независимо от их структуры и формата.

 Преимущества MapReduce
Параллельная обработка: Разделение задач и их выполнение одновременно на разных узлах (компьютерах), что экономит время.
Гибкость: Обработка как структурированных, так и неструктурированных данных.
Масштабируемость: Возможность работы с огромными объемами данных на множестве узлов.
Поддержка различных языков: Использование Java, Python, R и других языков.
 Примеры использования MapReduce
Социальные сети: Анализ активности пользователей (например, посещений профилей, взаимодействий с постами).
Рекомендательные системы: Netflix использует MapReduce для рекомендации фильмов на основе предпочтений пользователей.
Финансовые институты: Обнаружение аномалий в транзакциях, таких как мошеннические операции.
Рекламная индустрия: Анализ поведения пользователей при взаимодействии с рекламой (например, Google Ads).
Ключевые моменты:
MapReduce — это фреймворк для параллельных вычислений в распределенных системах.
Состоит из двух ключевых этапов: Map (создание пар "ключ-значение") и Reduce (агрегация данных).
Гибкость MapReduce позволяет обрабатывать любые типы данных и применять его в различных отраслях: от социальных сетей до финансов и рекламы.
Тема: Экосистема Hadoop
Введение
Экосистема Hadoop — это набор инструментов и программных компонентов, которые работают совместно для обработки больших данных (Big Data).

Основные и расширенные компоненты экосистемы Hadoop
Основные компоненты Hadoop:
Hadoop Common: Набор общих утилит и библиотек, поддерживающих другие модули Hadoop.
HDFS (Hadoop Distributed File System):
Хранилище для данных, которое распределяет их по нескольким узлам (нодам).
MapReduce:
Модель обработки данных, которая делает большие данные управляемыми за счет их параллельной обработки в кластерах.
YARN (Yet Another Resource Negotiator):
Управляет ресурсами между узлами кластера.
Расширенные компоненты экосистемы Hadoop:
Расширенная экосистема состоит из дополнительных библиотек и программного обеспечения, установленных поверх базовых компонентов. Эти компоненты разделены по этапам обработки данных.

Этапы экосистемы Hadoop
Экосистема Hadoop охватывает 4 основных этапа обработки данных:

Сбор данных (Ingest)
Хранение данных (Store)
Обработка и анализ данных (Process and Analyze)
Доступ к данным (Access)
1. Сбор данных (Ingest)
Когда данные поступают из различных источников, используются инструменты Flume и Sqoop:

Flume:
Инструмент для сбора, агрегации и передачи потоковых данных в систему хранения.
Имеет простую архитектуру и гибкий поток данных, подходит для аналитических приложений в реальном времени.
Sqoop:
Открытое программное обеспечение для передачи больших объемов данных между реляционными базами данных и Hadoop.
Автоматически создает MapReduce-код для импорта и экспорта данных.
2. Хранение данных (Store)
На этапе хранения используются:

HDFS:
Основная распределенная файловая система для хранения данных.
HBase:
Нереляционная, колоночная база данных, работающая поверх HDFS.
Использует хеш-таблицы для индексации данных и обеспечивает быстрый доступ.
Cassandra:
Масштабируемая NoSQL база данных, не имеющая единой точки отказа.
3. Обработка и анализ данных (Process and Analyze)
На этом этапе применяются такие инструменты, как Pig и Hive:

Pig:
Процедурный язык для анализа больших объемов данных.
Обеспечивает гибкость в обработке данных за счет пошаговых инструкций.
Hive:
Декларативный язык для создания отчетов.
Подходит для работы с серверной частью кластеров и позволяет пользователям задавать, какие данные нужны, без указания, как их получить.
4. Доступ к данным (Access)
После обработки данные становятся доступными для пользователей. Основные инструменты:

Impala:
Система, которая позволяет искать и получать доступ к данным в Hadoop без навыков программирования.
Hue:
Обеспечивает удобный интерфейс для работы с Hadoop:
Загрузка и просмотр данных.
Выполнение заданий Pig.
Использование SQL-редактора для выполнения запросов.
Пример работы экосистемы
Сбор данных:
Данные из социальных сетей (например, логины, лайки, комментарии) поступают через Flume в HDFS.

Хранение:
Эти данные сохраняются в HDFS или HBase для дальнейшего анализа.

Обработка и анализ:
Используя Hive, создается отчет о частоте взаимодействий пользователей с контентом.

Доступ:
Отчет становится доступным через Hue или Impala для принятия бизнес-решений.

Ключевые моменты:
Экосистема Hadoop делится на 4 этапа:

Сбор данных: Flume, Sqoop.
Хранение данных: HDFS, HBase, Cassandra.
Обработка данных: Pig, Hive.
Доступ к данным: Impala, Hue.
Каждый этап дополняет предыдущий, формируя мощный инструмент для работы с большими данными в различных отраслях.
Тема: Hadoop Distributed File System (HDFS)
Введение
HDFS (Hadoop Distributed File System) — это распределенная файловая система, которая является хранилищем данных в экосистеме Hadoop.

Что такое HDFS?
HDFS — это распределенная файловая система, которая:

Разделяет файлы на блоки, создает их реплики и хранит на разных машинах.
Обеспечивает доступ к данным через интерфейс командной строки.
Поддерживает потоковую передачу данных, обеспечивая постоянную скорость передачи.
Основные особенности HDFS
Дешевое оборудование: Использует недорогое оборудование для хранения данных, что снижает затраты.
Масштабируемость: Поддерживает кластеризацию с сотнями узлов.
Гибкость: Хранит данные любого формата (табличные, неструктурированные).
Отказоустойчивость: Благодаря репликации данных сохраняет их даже при сбоях оборудования.
Портативность: Легко переносится между различными платформами.
Основные концепции HDFS
Блоки данных:

Файл разбивается на небольшие части (блоки).
Размер блока может быть 64 или 128 МБ (по умолчанию).
Например, файл размером 500 МБ будет разделен на 3 блока по 128 МБ и один блок на 116 МБ.
Узлы (Nodes):

Имя-узел (NameNode):
Регулирует доступ к файлам, управляет заданиями для других узлов.
Узлы данных (DataNodes):
Хранят данные, выполняют операции чтения и записи по запросам NameNode.
Rack Awareness:

Rack — это группа из 40–50 узлов, подключенных к одному сетевому коммутатору.
NameNode выбирает узлы на одном или ближайшем стеллаже для уменьшения сетевого трафика.
Реплики данных распределяются по разным стеллажам для повышения надежности.
Репликация:

Создание копий блоков данных для резервирования.
Фактор репликации (по умолчанию 3) определяет количество копий каждого блока.
Например, файл из 4 блоков при факторе репликации 2 создаст 8 копий.
Архитектура HDFS
HDFS использует архитектуру "первичный/вторичный узел":

Имя-узел (NameNode):
Управляет операциями открытия, закрытия и переименования файлов.
Сопоставляет блоки файлов с узлами данных.
Узлы данных (DataNodes):
Отвечают за чтение и запись данных, создание, репликацию и удаление блоков.
Операции в HDFS
Чтение данных (Read):
Клиент отправляет запрос на NameNode, чтобы получить расположение блоков.
NameNode проверяет права доступа клиента и предоставляет список DataNodes.
Клиент связывается с ближайшим DataNode и читает данные.
После завершения работы клиент закрывает сессию.
Запись данных (Write):
Клиент отправляет запрос на NameNode для проверки прав доступа.
NameNode проверяет, существует ли файл. Если файл уже есть, клиент получает ошибку.
Если файл не существует, клиенту предоставляется разрешение на запись.
Данные записываются на DataNodes, создаются реплики, и процесс завершается подтверждением.
Преимущества HDFS
Экономичность: Использование дешевого оборудования снижает затраты.
Масштабируемость: Возможность добавления узлов для увеличения объема хранилища.
Отказоустойчивость: Репликация защищает данные от сбоев.
Эффективность: Rack Awareness уменьшает сетевой трафик и повышает производительность.
Удобство работы: Поддержка операций "один раз записать, много раз прочитать".
Пример работы HDFS
Сохранение данных:
Файл размером 500 МБ разбивается на блоки (например, 128 МБ), и каждый блок реплицируется на разные узлы.

Чтение данных:
Клиент получает местоположение блоков от NameNode и загружает данные с ближайшего DataNode.

Отказоустойчивость:
При сбое одного из узлов данные восстанавливаются из реплик.

Ключевые моменты:
HDFS — это надежная и масштабируемая система хранения данных.
Основные функции HDFS включают репликацию, Rack Awareness и операции "один раз записать, много раз прочитать".
Архитектура NameNode и DataNodes обеспечивает отказоустойчивость и высокую производительность системы.
HDFS идеально подходит для работы с большими данными и используется в различных отраслях для анализа и хранения информации.
Тема: Введение в Hive
Что такое Hive?
Hive — это программное обеспечение для работы с хранилищем данных (data warehouse) в экосистеме Hadoop. Оно используется для:

Чтения, записи и управления большими объемами данных.
Анализа данных табличного типа.
Хранилище данных в Hive сохраняет историческую информацию из различных источников, чтобы облегчить анализ данных и создание отчетов.

Особенности Hive:
Масштабируемость и скорость: Hive работает с петабайтами данных.
Простота в использовании: Если вы знакомы с SQL, вам будет легко изучить Hive, так как HiveQL основан на SQL.
Поддержка различных форматов файлов:
Sequence Files (пары "ключ-значение").
RCFiles (хранение данных в колонках).
Текстовые файлы и файлы без структуры.
Очистка и фильтрация данных: Hive позволяет выполнять предварительную обработку данных перед их анализом.
Сравнение Hive и традиционных RDBMS
Характеристика	RDBMS	Hive
Назначение	Управление базами данных	Управление хранилищами данных
Язык запросов	SQL	HiveQL (основан на SQL)
Тип анализа	Анализ в реальном времени	Статический анализ данных
Операции чтения и записи	Многоразовые операции	Запись один раз, многократное чтение
Объем обрабатываемых данных	До терабайтов	До петабайт
Проверка схемы	Схема проверяется до загрузки данных	Схема не проверяется до загрузки
Разделение данных	Может отсутствовать встроенная поддержка	Поддерживается разделение (partitioning)
Разделение данных (Partitioning):
Hive позволяет разбивать таблицы на части на основе значений определенного столбца, например, даты или города. Это улучшает производительность запросов.

Архитектура Hive
Hive состоит из трех основных компонентов:

Клиенты Hive (Hive Clients):
Поддерживаются драйверы для различных приложений:
JDBC: Для Java-приложений.
ODBC: Для других приложений.
Сервисы Hive (Hive Services):
Взаимодействуют с клиентами.
Выполняют запросы и управляют сессиями.
Хранят метаданные (информация о таблицах, их местоположении, схеме и т. д.).
Хранилище и вычисления Hive (Hive Storage and Computing):
Сохраняют метаданные в базах данных.
Загруженные данные и результаты запросов хранятся в HDFS.
Основные компоненты архитектуры Hive
Клиенты Hive:

JDBC-клиент: Подключает Java-приложения к Hive.
ODBC-клиент: Обеспечивает связь других приложений с Hive.
Сервисы Hive:
Сервер Hive: Обрабатывает запросы и поддерживает связь с клиентами через JDBC/ODBC.
Драйвер:
Получает запросы.
Инициализирует сессии.
Передает запрос компилятору.
Компилятор: Преобразует запросы в план выполнения.
Оптимизатор: Улучшает план выполнения запросов для повышения скорости и эффективности.
Исполнитель (Executor): Выполняет задачи после оптимизации.
Хранилище метаданных (Meta Store):

Центральное хранилище для информации о таблицах, их местоположении и структуре.
Принципы работы Hive
Чтение данных:

Клиент отправляет запрос на сервер Hive через драйвер.
Сервер передает запрос компилятору и оптимизатору, которые создают план выполнения.
Данные извлекаются из хранилища и передаются клиенту.
Запись данных:

Клиент загружает данные в таблицы Hive.
Данные сохраняются в HDFS, а информация о таблицах обновляется в метахранилище.
Ключевые моменты:
Hive — это инструмент для управления хранилищем данных и анализа больших объемов информации.
HiveQL делает работу удобной для пользователей, знакомых с SQL.
Архитектура Hive включает клиентов, сервисы и хранилища, которые обеспечивают эффективное управление данными.
Поддержка больших объемов данных, разделение и масштабируемость делают Hive идеальным решением для анализа данных в Hadoop.
Тема: Архитектура Apache Spark
Apache Spark — это мощная распределённая вычислительная платформа для обработки больших данных. Он состоит из нескольких ключевых компонентов, которые работают вместе для обеспечения масштабируемости и отказоустойчивости. В этом уроке мы рассмотрим архитектуру Spark, её основные компоненты и принципы работы.

 Основные компоненты архитектуры Apache Spark
Архитектура Apache Spark состоит из трёх основных компонентов:

Хранение данных
Данные загружаются из различных источников в память для обработки. Spark поддерживает любые хранилища, совместимые с Hadoop (например, HDFS, S3 и другие).

Высокоуровневые программные API
Spark предоставляет API для различных языков программирования: Scala, Python и Java. Эти API используются для создания приложений, которые могут обрабатывать данные на кластере Spark.

Фреймворк управления кластерами
Этот компонент отвечает за управление распределёнными вычислениями и масштабирование. Spark может работать с разными фреймворками управления кластерами, такими как YARN, Mesos или встроенный Spark Standalone Cluster Manager. Этот фреймворк управляет распределением задач и ресурсов на узлах кластера.

Представьте, что данные находятся в файловой системе Hadoop, и Spark загружает их в память для обработки. Далее, через API, данные направляются на различные узлы кластера для выполнения параллельных задач.

Spark Core: Базовый движок
Spark Core — это ядро Apache Spark, которое отвечает за базовые операции и функциональность.

 Это отказоустойчивый движок, который выполняет параллельную и распределённую обработку данных на больших масштабах. Основные задачи Spark Core:

Управление памятью. Spark управляет распределённым хранением данных в памяти для ускорения вычислений.
Планирование задач. Spark делит вычислительные задачи на меньшие задания (tasks), которые затем выполняются на кластере.
Работа с API для определения RDD (Resilient Distributed Datasets) и других типов данных
Обработка ошибок. Spark Core обеспечивает отказоустойчивость за счёт сохранения информации о данных и выполнения вычислений.
Spark Core распределяет данные по узлам кластера и координирует выполнение задач.

 Архитектура Spark-приложений
Каждое Spark-приложение состоит из двух основных программ:

Драйвер — это программа, которая управляет выполнением приложения. Она запускает задачи, распределяет их по исполнителям и собирает результаты. Драйвер работает на основном узле кластера.

Исполнители — программы, которые выполняют задачи на рабочих узлах кластера. Каждый исполнитель может использовать несколько ядер для параллельных вычислений. Исполнители обрабатывают данные, распределённые(назначенные) драйвером.

Под задачи драйвера входят:

Запуск Spark-программ: драйвер инициирует создание RDD и запуск действий.
Отправка задач: драйвер разбивает работу на задачи и распределяет их между исполнителями на рабочих узлах.
Сбор результатов: драйвер собирает результаты выполнения задач и выполняет окончательные вычисления.
Под задачи исполнителя входят:

Распределённая обработка: каждый исполнитель работает с частью данных, обрабатывая её параллельно с другими исполнителями.
Хранение данных: исполнители хранят данные в памяти и на диске, обеспечивая быстрый доступ к данным для дальнейших вычислений.
Отправка результатов: исполнители отправляют результаты обратно на драйвер по завершении выполнения задачи.
 Worker Nodes
Рабочие узлы — это физические или виртуальные машины, на которых запускаются процессы исполнителей. Каждый рабочий узел выполняет определённое количество задач (tasks), получает данные и отправляет результаты обратно в драйвер.

Если требуется обработать больше данных, вы можете добавить дополнительные рабочие узлы для масштабирования вычислений.

Масштабирование с Apache Spark
Spark позволяет масштабировать обработку данных за счёт добавления новых рабочих узлов. Эти узлы могут использоваться для распределения данных и задач между большим количеством вычислительных ресурсов. Это обеспечивает быструю обработку больших объёмов данных в реальном времени.
Взаимодействие компонентов:
Драйвер и исполнители: Драйвер — это центр управления вычислениями. Он делит задачи и отправляет их на исполнителей. Каждый исполнитель обрабатывает свою часть данных и возвращает результат обратно на драйвер.
RDD: Spark использует RDD как основную абстракцию данных. Данные могут быть как изначально загружены в RDD, так и созданы путём трансформаций других RDD.
Кластерный менеджер: Этот компонент управляет ресурсами в кластере (например, CPU, память), распределяет их между драйвером и исполнителями и гарантирует выполнение задач с учётом доступных ресурсов.
Основные этапы взаимодействия:
Инициализация приложения: Драйвер запускает приложение и создаёт RDD.
Распределение задач: Драйвер делит задачи и отправляет их исполнителям через кластерный менеджер.
Обработка данных: Исполнители обрабатывают данные параллельно на разных узлах кластера.
Сбор результатов: После завершения задач исполнители отправляют результаты обратно в драйвер, который может выполнить финальную агрегацию или сохранение данных.


Важные особенности:
Параллельная обработка: Spark использует параллельные вычисления для распределённой обработки данных, что позволяет обрабатывать большие объёмы данных быстро.
Отказоустойчивость: Spark обеспечивает отказоустойчивость, благодаря использованию RDD, которые сохраняют данные в памяти и могут быть восстановлены в случае сбоя.
Гибкость: Spark работает с различными типами данных и поддерживает множество источников, таких как HDFS, S3, Cassandra и другие.
Тема: Параллельная  обработка в Apache Spark
Распределенные вычисления:
Это тип вычислений, при котором задачи или процессы распределяются между несколькими независимыми компьютерами (или узлами) в сети, которые взаимодействуют друг с другом для выполнения общей задачи.

В распределенных вычислениях каждый узел может работать с собственным набором данных, и они могут обмениваться информацией, чтобы решить сложную задачу.
Основное свойство: Распределение работы между несколькими машинами. Каждый узел может работать независимо, но они должны быть синхронизированы для выполнения общей задачи.
В Apache Spark распределенная обработка данных происходит на множестве узлов (машин). Каждый узел обрабатывает часть данных, и результаты затем объединяются для получения окончательного ответа.(см. рис.)


Распределенные системы:
Это системы, состоящие из нескольких компьютеров или узлов, которые могут работать независимо, но взаимодействуют друг с другом для выполнения определенных задач. Эти узлы могут находиться в одной физической локации или быть распределены по всему миру.

Важно, что такие системы обеспечивают согласованность, избыточность и отказоустойчивость, чтобы они могли продолжать работать, если один из узлов выходит из строя.

Основное свойство: Система, состоящая из нескольких узлов, которые могут быть физически распределены.
 Кластеры данных (например, Hadoop, Apache Spark) являются распределенными системами, где каждый узел выполняет часть работы, а система в целом управляет большими объемами данных.(см. рис.)
Параллельные вычисления:
Это вычисления, при которых одна и та же задача или процесс делится на несколько подзадач, которые выполняются одновременно.

В отличие от распределенных вычислений, параллельные вычисления происходят на одной машине с несколькими процессорами или ядрами. Они могут быть реализованы как на многозадачных процессорах (например, многозадачные CPU) или с помощью технологий, таких как GPU.

Основное свойство: Одновременное выполнение нескольких операций на одном узле (или на нескольких ядрах процессора).
 Выполнение сложных математических операций на GPU, где тысячи ядер одновременно обрабатывают часть данных для ускорения вычислений.
Параллельная распределенная обработка:
Это комбинированный подход, при котором вычисления как параллельны (внутри отдельных узлов), так и распределены между множеством узлов.

В этой модели задачи делятся не только между несколькими машинами, но и между несколькими процессорами внутри каждой машины. Это позволяет эффективно использовать ресурсы и ускорить выполнение задач.

Основное свойство: Одновременная параллельная обработка на нескольких узлах, где каждый узел может использовать несколько процессоров или ядер для выполнения подзадач.
 В Apache Spark данные распределяются по узлам, а внутри каждого узла параллельно обрабатываются части данных
Сравнение и различия:
Термин	Описание	Пример
Распределенные вычисления	Процессы или задачи распределяются по множеству машин (узлов). Каждый узел работает с собственными данными.	Spark, Hadoop, распределенная база данных.
Распределенные системы	Система, состоящая из нескольких независимых узлов, которые взаимодействуют для решения общей задачи.	Кластеры, облачные сервисы (AWS, Azure, Google Cloud).
Параллельные вычисления	Одна задача делится на несколько подзадач, которые выполняются одновременно на одном процессоре или ядре.	Многозадачные вычисления на одном компьютере, вычисления на GPU.
Параллельная распределенная обработка	Задачи делятся на части, которые обрабатываются одновременно как на нескольких машинах, так и внутри них.	Обработка данных в Spark, где задачи распределяются по узлам, а внутри каждого узла параллельно обрабатываются данные.
Ключевые моменты:
Распределенные вычисления и распределенные системы фокусируются на разделении задачи между несколькими машинами.
Параллельные вычисления фокусируются на разделении задачи внутри одного узла.
Параллельная распределенная обработка сочетает оба подхода, позволяя и распределять данные по узлам, и использовать параллельные вычисления внутри каждого узла.
Тема: Apache Spark 
Apache Spark - это распределенная вычислительная система с открытым исходным кодом, которая используется для обработки больших объемов данных. Это мощный инструмент для анализа данных, который может работать с различными источниками данных, такими как HDFS (Hadoop Distributed File System), базы данных, файлы, и другие. Spark разработан так, чтобы быть быстрее и удобнее в использовании, чем старые системы, такие как Hadoop MapReduce.
Как работает Apache Spark:
Шаг 1: Загрузка данных
Данные поступают из различных источников (файлы, базы данных, HDFS)
Данные преобразуются в RDD (Resilient Distributed Dataset) или DataFrame
Шаг 2: Driver Program
Главный процесс, который управляет всей работой
Создает план выполнения задач
Распределяет задачи между исполнителями
Шаг 3: Cluster Manager
Управляет ресурсами кластера
Распределяет ресурсы между задачами
Шаг 4: Executors
Выполняют назначенные задачи
Обрабатывают данные параллельно
Хранят промежуточные результаты в памяти

Рисунок 1 - Общая схема работы - Apache Spark

Ключевые особенности:
Ленивые вычисления: операции выполняются только когда нужен результат
Распределенная обработка: данные обрабатываются параллельно на разных узлах
Отказоустойчивость: система может восстановиться после сбоев
Кэширование в памяти: промежуточные результаты хранятся в памяти для быстрого доступа
Пример работы распределенной системы
Cхема работы распределенной системы рассмотрена в данном уроке  Шаг 1 (см. рис.)

Представим, что  нужно обработать большой набор данных - посчитать среднюю температуру за год по данным с разных метеостанций.

Преимущества распределенных вычислений
Масштабируемость
Легко добавлять новые машины
Горизонтальное масштабирование
Отказоустойчивость
Система продолжает работать даже при сбое отдельных машин
Пример: из 8 машин могут работать 6, и система все равно функционирует
Рисунок 3 - Пример работы распределенной системы
В этом примере:

Данные распределяются между разными исполнителями (executors)
Каждый executor обрабатывает свою часть данных в памяти
Даже если один executor выйдет из строя, задача может быть перераспределена
Все вычисления происходят параллельно, что ускоряет обработку
Тема: Apache Spark и распределенные вычисления
Ключевые концепции:
Apache Spark: фреймворк с открытым исходным кодом для распределенной обработки больших данных

In-Memory Processing: обработка данных в оперативной памяти (RAM)

Распределенные вычисления: группа компьютеров, работающих совместно, где каждый процессор использует свою память

Параллельные вычисления: вычисления с общей памятью для всех процессоров

Преимущества Apache Spark:
Скорость обработки: работа с данными в памяти вместо записи на диск
Масштабируемость: возможность горизонтального масштабирования
Отказоустойчивость: продолжение работы при сбое отдельных узлов
Универсальность: поддержка различных языков программирования (Python, Scala, Java)
Сравнение с MapReduce:
Характеристика	Apache Spark	MapReduce
Хранение данных	В памяти (RAM)	На диске (HDFS)
Скорость обработки	Высокая	Низкая
Операции ввода-вывода	Минимальные	Частые
Apache Spark решает проблему с чтением и записью данных в традиционной задаче MapReduce:
Работа Apache Spark с данными и решения проблемы время-затратных операций чтения и записи в традиционной задаче MapReduce, рассмотрим на схеме, которая покажет, как Spark решает проблемы, связанные с использованием памяти и избеганием операций с диском.
Схема будет включать следующие элементы:

MapReduce: Чтение и запись данных на диск или HDFS, что затратно по времени.
Apache Spark: Сохранение данных в памяти и уменьшение операций с диском.
Решение проблемы: Уменьшение времени обработки благодаря хранению данных в памяти.
Использование Spark для обработки больших данных, включая:
Инструменты Spark: SparkSQL, DataFrames, SparkML, Streaming.
Машинное обучение и обработка данных.
Тема: Функциональное программирование
Apache Spark - фреймворк для обработки больших данных, который использует функциональное программирование и параллельные вычисления для эффективной работы с данными.

Одной из ключевых особенностей Spark является возможность параллельной обработки данных с использованием RDD (Resilient Distributed Datasets) или DataFrame, что очень хорошо сочетается с парадигмой функционального программирования.

Apache Spark активно использует принципы функционального программирования, такие как чистые функции, неизменяемость данных и функции первого класса. Эти концепции позволяют легко работать с большими объемами данных в распределенной среде, при этом избегая сложных синхронизаций и проблем с изменяемостью состояния.

Лямбда-исчисление в Apache Spark
Лямбда-выражения (анонимные функции) широко используются в Spark для описания трансформаций и операций над данными. Например, в операции map, filter, reduce и других можно использовать лямбда-выражения для определения того, как обрабатывать каждый элемент данных.

Функции, которые передаются в Spark API, фактически являются лямбда-функциями, которые могут быть вызваны на каждом элементе данных в распределенной среде.

Преимущества использования лямбда-выражений в Spark:
Удобство: Лямбда-функции позволяют создавать короткий и читаемый код для обработки данных.
Производительность: Spark автоматически распределяет выполнение операций, что позволяет эффективно работать с большими объемами данных на множестве узлов.
Отсутствие побочных эффектов: Использование чистых функций гарантирует, что данные не будут изменяться вне контроля, что снижает вероятность ошибок при параллельной обработке.
Как лямбда-исчисление связано с параллельной обработкой в Spark?
В Spark лямбда-выражения могут быть использованы в операциях, которые выполняются над распределёнными данными, такими как:
map(): применяет функцию к каждому элементу данных.
filter(): фильтрует элементы данных на основе функции.
reduce(): объединяет элементы данных в одно значение.
flatMap(): трансформирует каждый элемент в несколько других элементов.
Все эти операции параллелятся и могут быть выполнены на разных узлах кластера, используя принципы функционального программирования:
Функции первого класса позволяют передавать лямбда-выражения в качестве аргументов.
Чистые функции обеспечивают, что данные не будут изменяться, что помогает избежать ошибок в многозадачных вычислениях.
Неизменяемость данных позволяет безопасно работать с большими объемами информации без необходимости синхронизации данных между потоками или процессами.
Тема: Функциональное программирование. 
Пример использования лямбда-выражений в Spark
Пример с использованием RDD:
Задача: Есть файл с данными о покупках пользователей, и  нужно:
Подсчитать общую сумму покупок для каждого пользователя.
Отсортировать пользователей по величине их расходов.
Предположим, что данные в формате CSV, где каждый элемент строки имеет структуру: user_id, item, price, quantity.
Реализация с использованием Spark RDD и лямбда-выражений
Мы создаем RDD из исходных данных.
Затем применяем map() для вычисления стоимости каждой покупки (price * quantity).
Используем reduceByKey() для агрегации покупок по пользователю.
Применяем sortBy() для сортировки пользователей по общей сумме.
Разбор работы программы:
Исходные данные — это список кортежей (user_id, item, price, quantity), представляющих покупки пользователей.
map(lambda x: (x[0], x[2] * x[3])):
Для каждой покупки вычисляется стоимость (цена товара умножается на количество).
Каждое значение x представляет собой кортеж из четырех элементов: (user_id, item, price, quantity).
Мы создаем пару (user_id, стоимость_покупки).
reduceByKey(lambda x, y: x + y):
Эта операция агрегирует все покупки одного пользователя, суммируя их общую стоимость.
Для каждого уникального user_id мы складываем все соответствующие стоимости покупок.
sortBy(lambda x: x[1], ascending=False):

Мы сортируем результаты по второй части пары — общей сумме покупок, в порядке убывания (самый большой расход на первом месте).
collect():

Мы собираем результаты и выводим их на экран.
Ключевые моменты:
Лямбда-выражения здесь используются для:
Преобразования данных (вычисление стоимости каждой покупки).
Суммирования покупок пользователя в reduceByKey.
Сортировки пользователей по сумме покупок в sortBy.
Параллельная обработка: Вся обработка данных происходит параллельно на разных узлах кластера, что делает этот подход масштабируемым и эффективным для работы с большими данными.
Тема: Реализация с использованием Spark DataFrame и лямбда-выражений
Теперь рассмотрим, как решить ту же задачу что в Шаг 2, используя DataFrame API
Разбор работы программы:
Создание DataFrame: Необходимо создать DataFrame из исходных данных. Каждый элемент в DataFrame представляет собой запись с полями: user_id, item, price, quantity.

Вычисление стоимости покупки: Добавляем новый столбец total_price для вычисления стоимости каждой покупки, умножив цену на количество товара. Это делается с помощью withColumn(), где используется выражение col("price") * col("quantity").

Группировка и агрегация: Группируем данные по user_id с помощью метода groupBy() и вычисляем сумму стоимости всех покупок каждого пользователя с помощью функции sum().

Сортировка: Сортируем пользователей по сумме их покупок с помощью метода orderBy(), указывая на поле sum(total_price).

Вывод результата: Используется метод show() для вывода результата.

Ключевые моменты:
Использование RDD:

В RDD  работаем с низкоуровневыми преобразованиями, такими как map, reduceByKey, и sortBy. Лямбда-выражения применяются для выполнения операций на каждом элементе данных.
RDD предоставляет большую гибкость и контроль, но требует больше кода для реализации.
Использование DataFrame:

DataFrame API обеспечивает более высокоуровневый, декларативный способ работы с данными, что позволяет писать меньше кода и автоматически оптимизировать выполнение через Catalyst Optimizer.
DataFrame идеально подходит для работы с табличными данными и часто используется в аналитике больших данных.
Обе реализации решают одну и ту же задачу, но с разной степенью абстракции и удобства.
Тема: Параллельное программирование
Что такое RDD (Resilient Distributed Dataset)?
RDD (Resilient Distributed Dataset) — это основная абстракция данных в Apache Spark, представляющая собой распределённый набор данных, который может быть обработан параллельно на разных узлах кластера.
Устойчивость: RDD обеспечивают отказоустойчивость, так как данные автоматически восстанавливаются в случае сбоя.
Неизменяемость: После создания RDD нельзя изменить его данные. Операции над RDD создают новые RDD.
Распределённость: Данные в RDD распределены по узлам кластера, что позволяет выполнять параллельные вычисления.
Способы создания RDD
Есть несколько способов создать RDD в Apache Spark:

Способ 1: Создание RDD из существующей коллекции (например, списка или массива)
Используя метод parallelize, можно создать RDD из коллекции данных, находящейся на локальной машине или в драйвере.

Предположим, у нас есть небольшой список заказов в интернет-магазине, и мы хотим посчитать общее количество товаров в заказах.

from pyspark import SparkContext

sc = SparkContext("local", "RDD Example")

# Список заказов (ID пользователя, товар, количество)
orders = [
    (1, "Laptop", 1),
    (2, "Smartphone", 2),
    (3, "Tablet", 1),
    (4, "Headphones", 3),
    (5, "Keyboard", 1)
]
# Создаём RDD из локальной коллекции
rdd = sc.parallelize(orders)

# Применяем map для получения только количества товаров
quantities = rdd.map(lambda x: x[2])

# Суммируем все количества товаров
total_quantity = quantities.reduce(lambda x, y: x + y)

# Выводим результат
print("Общее количество товаров:", total_quantity)  # 8
parallelize(): Эта функция создаёт RDD из локальной коллекции данных (списка, массива и т.д.), которая будет распределена по кластеру.

Мы создали RDD из локального списка заказов.
Применили map(), чтобы извлечь только количество товаров из каждого заказа.
Использовали reduce(), чтобы сложить все количества товаров.
Способ 2: Создание RDD из внешнего файла (например, текстового файла)
RDD можно создать, загрузив данные из файла, поддерживаемого Hadoop (например, HDFS, локальная файловая система).

Предположим, у нас есть текстовый файл с логами, и мы хотим посчитать количество строк в этом файле.

Пример:

Создадим файл log.txt с таким содержимым:
INFO User 1 logged in
ERROR User 2 failed login
INFO User 3 logged in
ERROR User 1 failed login
INFO User 2 logged in
 2. Создадим RDD из этого файла и посчитаем количество строк, содержащих "ERROR".

# Создание RDD из текстового файла
log_rdd = sc.textFile("log.txt")

# Применяем filter, чтобы выбрать только строки с ошибками
error_logs = log_rdd.filter(lambda line: "ERROR" in line)

# Считаем количество таких строк
error_count = error_logs.count()

# Выводим результат
print("Количество ошибок в логах:", error_count)  # 2
Способ 3: Преобразование другого RDD с помощью трансформаций

Можно создать новый RDD, применив трансформации (например, map(), filter()) к существующему RDD.

Предположим у  нас есть список товаров с их ценами и категориями. Мы хотим отсортировать эти товары по цене от самой высокой к самой низкой.

from pyspark import SparkContext

sc = SparkContext("local", "Product Sort Example")

# Список товаров (название товара, категория, цена)
products = [
    ("Laptop", "Electronics", 1000),
    ("Smartphone", "Electronics", 500),
    ("Keyboard", "Accessories", 100),
    ("Tablet", "Electronics", 600),
    ("Headphones", "Accessories", 150)
]

# Создаём RDD из списка товаров
rdd = sc.parallelize(products)

# Применяем sortBy для сортировки товаров по цене в порядке убывания
sorted_rdd = rdd.sortBy(lambda x: x[2], ascending=False)

# Выводим отсортированные товары
print(sorted_rdd.collect())
rdd.sortBy() — используется для сортировки RDD. В нашем случае мы сортируем товары по третьему элементу в кортежах (цене товара). Параметр ascending=False указывает, что сортировка должна быть в порядке убывания (от самой высокой цены к самой низкой).

Мы используем lambda x: x[2] для того, чтобы указать, что сортировка должна происходить по цене, которая находится в третьем элементе каждого кортежа.

В результате мы получаем список товаров, отсортированный по цене в порядке убывания.

Используется трансформация sortBy(), чтобы отсортировать товары по их цене.

sortBy() позволяет эффективно сортировать данные в RDD на основе любого поля.

Это пример того, как можно обрабатывать данные с помощью трансформаций в RDD в Spark.

Способ 4: Создание RDD с использованием внешней базы данных (например, Cassandra, HBase)
Spark поддерживает создание RDD из данных, хранящихся в реляционных и NoSQL базах данных, таких как Cassandra, JDBC и т.д.

Предположим, у нас есть база данных с информацией о продуктах, и мы хотим извлечь эту информацию и создать RDD для анализа.

Для этого нужно использовать Spark Cassandra Connector, и пусть уже настроено соединение с Cassandra.

# Создание RDD из таблицы в Cassandra ( таблица "products" существует в Cassandra)
product_rdd = sc.cassandraTable("ecommerce", "products")

# Применяем map, чтобы извлечь только названия продуктов
product_names = product_rdd.map(lambda x: x["name"])

# Выводим все названия продуктов
print(product_names.collect())
Ключевые моменты:
Создание RDD из локальной коллекции: Полезно, когда данные уже находятся в памяти (например, список заказов).
Создание RDD из текстового файла: Подходит для анализа данных в текстовом формате (например, логи).
Преобразование другого RDD с помощью трансформаций: Используется для манипуляций с данными, например, для сортировки или фильтрации.
Создание RDD с использованием внешних источников данных (например, Cassandra): Подходит для работы с данными, хранящимися в распределённых базах данных.
Каждый из этих способов подходит для разных типов задач, и позволяет работать с большими объёмами данных эффективно в распределённой среде с использованием параллельных вычислений.
Тема: Параллельное программирование
Как связаны параллельное программирование и RDD?
Параллельное программирование — это метод программирования, при котором вычисления выполняются одновременно на нескольких процессорах или узлах. Это позволяет ускорить обработку больших объёмов данных.

Связь параллельного программирования и RDD:

Распределённые вычисления: RDD позволяет эффективно использовать параллельные вычисления на кластере Spark. Каждый раздел RDD обрабатывается на отдельном узле кластера, что позволяет ускорить выполнение вычислений за счёт параллельной обработки.
Деление на разделы: RDD делится на разделы (partitions), которые могут быть обработаны параллельно. Количество разделов зависит от конфигурации кластера и задачи.
Поддержка параллельных операций: Все операции над RDD (например, map, filter, reduce, groupBy) автоматически выполняются параллельно, если данные разделены на несколько частей. Spark сам управляет распределением данных и параллельной обработкой.
Обработка данных в параллели: RDD позволяют выполнять параллельные операции над большими объёмами данных, что значительно ускоряет обработку по сравнению с последовательными вычислениями.
Что такое ETL?
ETL — это аббревиатура от Extract (извлечение), Transform (преобразование) и Load (загрузка).

Этот процесс широко используется в инженерии данных и аналитике, чтобы подготовить информацию для последующего анализа, отчетности и принятия решений.

Представьте, что у вас есть данные из разных источников: базы данных, веб-сайты, файлы Excel или API-сервисы. Эти данные могут быть в разных форматах и структурах, содержать ошибки или пропуски. ETL позволяет собрать, очистить и привести эти данные к единому стандарту, а затем загрузить в хранилище для удобного использования.

Зачем нужен ETL?

Данные редко бывают идеально структурированными. Их нужно фильтровать, исправлять ошибки и дополнять.
Компании работают с данными из множества источников (CRM, ERP, соцсети, IoT-устройства).
Данные должны быть доступны для аналитиков, машинного обучения и бизнес-приложений.
Извлечение данных (Extract)
Извлечение — это первый шаг ETL, который заключается в сборе данных из различных источников.

Какие бывают источники данных?
🔹 Базы данных (MySQL, PostgreSQL, MongoDB)
🔹 Файлы (CSV, JSON, Excel, XML)
🔹 API и веб-сервисы (например, получение данных о погоде или курсах валют)
🔹 Веб-скрейпинг (парсинг данных с веб-страниц)
🔹 Датчики и IoT-устройства (умные часы, камеры наблюдения, сенсоры)

Извлечение данных может выполняться пакетно (например, один раз в день) или в реальном времени (например, обработка финансовых транзакций).

Преобразование данных (Transform)
После извлечения данные могут содержать дубликаты, ошибки, отсутствующие значения или быть в неудобном формате. Преобразование — это этап, на котором данные очищаются, реструктурируются и подготавливаются для загрузки.

Какие преобразования могут выполняться?
✅ Очистка данных (удаление дубликатов, исправление ошибок)
✅ Приведение к единому формату (например, объединение данных с разными единицами измерения)
✅ Заполнение пропусков (например, средними значениями или предсказанными данными)
✅ Объединение данных из разных источников
✅ Создание новых признаков (например, расчет среднего чека клиента)

Этот этап особенно важен, если данные используются для бизнес-анализа, машинного обучения или финансовой отчетности.

Загрузка данных (Load)
На последнем этапе обработанные данные записываются в целевую систему, например:
📊 Хранилище данных (Data Warehouse)
📊 Витрина данных (Data Mart)
📊 Аналитическая база (OLAP)
📊 BI-инструменты (Tableau, Power BI)

Загрузка данных может быть полной (весь набор данных записывается заново) или инкрементальной (записываются только новые или измененные данные).

Где применяется ETL?
📌 Бизнес-аналитика
Компании используют ETL для объединения данных из CRM, ERP, маркетинговых систем и создания дашбордов для принятия решений.

📌 Финансы и банковская сфера
ETL помогает анализировать транзакции, обнаруживать мошенничество и подготавливать данные для отчетности.

📌 Интернет-магазины и маркетплейсы
Сбор и анализ данных о покупателях, заказах, логистике и ценах на товары.

📌 Машинное обучение и AI
ETL позволяет подготовить большие объемы данных для обучения моделей, например, предсказания спроса или автоматической классификации.

📌 IoT и умные города
Обработка данных с датчиков, видеокамер, умных домов и транспортных систем.

Ключевые моменты:
📌 ETL — это процесс извлечения, преобразования и загрузки данных в хранилище или аналитику.
📌 Извлечение данных включает доступ к источникам и их считывание.
📌 Преобразование данных включает очистку, фильтрацию, объединение и подготовку данных.
📌 Загрузка данных делает их доступными для аналитики и машинного обучения.
📌 ETL применяется в бизнесе, финансах, интернет-магазинах, AI, IoT и других сферах.
Что такое ETL и ELT?
ETL (Extract, Transform, Load) и ELT (Extract, Load, Transform) — это два подхода к обработке данных. Они используются в аналитике, хранилищах данных и больших данных, но выполняют процессы в разном порядке.

🔹 ETL: Данные сначала извлекаются (Extract), затем преобразуются (Transform) и только потом загружаются в хранилище (Load).
🔹 ELT: Данные сначала извлекаются (Extract), затем загружаются в хранилище (Load), а уже после этого преобразуются (Transform).

Главное отличие заключается в том, где выполняется этап преобразования данных:
В ETL — до загрузки в хранилище.
В ELT — после загрузки в хранилище.

Ключевые различия между ETL и ELT

Где выполняются преобразования?	До загрузки в хранилище	Внутри хранилища данных
Где хранятся "сырые" данные?	Не сохраняются, обрабатываются заранее	Сохраняются в хранилище
Гибкость	Жесткий процесс, сложно менять	Гибкий, можно анализировать "сырые" данные
Масштабируемость	Ограничена вычислительными мощностями	Использует облачные технологии, легко масштабируется
Время обработки	Данные подготавливаются заранее, могут быть задержки	Доступ к данным сразу после загрузки
Какие данные поддерживаются?	В основном структурированные (SQL, CSV)	Любые (SQL, JSON, NoSQL, изображения, видео)
Где используется?	Традиционные BI-системы, отчеты, финансовые аналитики	Data lakes, Big Data, машинное обучение, AI
 
Когда использовать ETL, а когда ELT?
ETL подходит, если:
✅ Данные должны быть предварительно очищены и преобразованы перед использованием.
✅ Вы работаете с структурированными данными (например, банковские транзакции).
✅ Требуется жесткий контроль за качеством данных перед их загрузкой.
✅ Используется локальная инфраструктура , а не облако.

ELT подходит, если:
✅ Нужно быстро загружать данные, а анализ проводить позже.
✅ Вы работаете с неструктурированными данными (JSON, NoSQL, аудио, видео).
✅ У вас облачная среда с мощными инструментами обработки.
✅ Требуется гибкость в анализе данных без предварительной трансформации.

Ключевые моменты:

📌 ETL — это классический метод обработки данных, который подходит для строгих BI-аналитических процессов.
📌 ELT — это более современный подход, который используется в больших данных, облачных технологиях и AI.
📌 Выбор между ETL и ELT зависит от инфраструктуры, типа данных и бизнес-задач.
Что такое извлечение данных?
Извлечение данных — это первый этап процессов ETL (Extract, Transform, Load) и ELT (Extract, Load, Transform). На этом этапе данные загружаются из различных источников для дальнейшей обработки и анализа.

Ключевые аспекты:
 Источники данных могут быть структурированными (базы данных, хранилища) и неструктурированными (файлы, веб-страницы, логи).
Извлечение может выполняться в режиме пакетной обработки (batch) или в реальном времени (streaming).
 В ETL данные сначала извлекаются и трансформируются, а затем загружаются в хранилище.
 В ELT данные извлекаются и загружаются в хранилище без предварительной трансформации, а преобразования выполняются уже в системе хранения.

Методы извлечения данных
Существует несколько способов извлечения данных в рамках ETL и ELT.

1. Прямое подключение к базам данных
 Используется для извлечения структурированных данных с помощью SQL-запросов.
 Поддерживает реляционные базы (PostgreSQL, MySQL, SQL Server, Oracle) и NoSQL-хранилища (MongoDB, Cassandra).
Применяется в ETL для выборки данных перед преобразованием.
 В ELT данные загружаются в хранилище в исходном виде.

2. Извлечение через API
 Позволяет получать данные из облачных сервисов, CRM-систем, соцсетей.
Часто используется для динамических и потоковых данных.
 Применяется как в ETL, так и в ELT, но в ELT данные могут загружаться без предварительной очистки.

3. Веб-скрейпинг
 Извлечение данных с веб-страниц с помощью инструментов (BeautifulSoup, Scrapy).
 Используется для сбора данных из открытых источников, включая новости, цены, пользовательские отзывы.
Применяется в ETL, если данные нужно очистить перед загрузкой, и в ELT, если сохраняются в необработанном виде.

4. Чтение файлов (CSV, JSON, XML, Parquet)
Позволяет загружать данные из логов, отчетов, экспортов из систем.
 Широко применяется в ETL для обработки и нормализации данных перед загрузкой.
 В ELT файлы могут просто копироваться в хранилище без преобразования.

5. Потоковая обработка данных (Kafka, Flink, Spark Streaming)
 Используется для извлечения данных в реальном времени (IoT, логи, финтех).
 В ETL чаще применяется для подготовки данных перед загрузкой.
 В ELT данные поступают в хранилище в необработанном виде, а преобразования выполняются по мере необходимости.

6. Извлечение из хранилищ данных (Data Warehouses, Data Lakes)
 Подходит для работы с большими массивами исторических данных.
 В ETL это обычно вспомогательный этап (например, объединение с новыми данными).
 В ELT данные загружаются в Data Lake, а затем анализируются пользователями и BI-системами.

Применение методов извлечения данных
🔹 Финансовая аналитика
Источники: транзакции, банковские системы, API фондовых бирж.
Методы: подключение к базам, API, потоковая обработка.

🔹 Маркетинг и социальные сети
Источники: соцсети, веб-аналитика, CRM-системы.
Методы: API, веб-скрейпинг, файлы CSV.

🔹 Индустриальный IoT
Источники: сенсоры, умные устройства, лог-файлы.
Методы: потоковая обработка, API, граничные вычисления.

🔹 Обучение моделей машинного обучения
Источники: корпоративные базы данных, Data Lakes, файлы JSON.
Методы: SQL-запросы, API, файловый ввод.

Ключевые моменты:
📌 Извлечение данных — первый шаг в ETL и ELT, определяющий дальнейший процесс обработки.
📌 Методы извлечения зависят от типа данных (структурированные/неструктурированные, статические/потоковые).
📌 ETL больше ориентирован на предварительную обработку данных, тогда как ELT позволяет загружать "сырые" данные и трансформировать их по мере необходимости.
📌 Выбор метода зависит от задачи: SQL и API удобны для отчетности, потоковая обработка нужна для анализа событий в реальном времени.
Что такое трансформация данных?
Трансформация данных — это процесс изменения их структуры, формата или значений с целью подготовки к анализу или загрузке в хранилище.

Основные цели трансформации:
🔹 Обеспечение качества данных (очистка, нормализация).
🔹 Приведение к требуемому формату.
🔹 Оптимизация для аналитики и хранения.
🔹 Объединение данных из разных источников.

Методы трансформации данных
Существует множество методов преобразования данных, но ключевые из них включают:

Очистка данных – удаление дубликатов, исправление ошибок, заполнение пропущенных значений.
Приведение типов данных – преобразование значений в нужные форматы (числовые, строковые, категориальные).
Фильтрация данных – удаление ненужных записей по заданным критериям.
Агрегация данных – объединение данных (например, расчет средних значений или суммирование).
Нормализация данных – приведение к единому масштабу (например, конвертация валют, единиц измерения).
Обогащение данных – добавление информации из внешних источников.
Анонимизация и шифрование – защита конфиденциальных данных.
Объединение данных – соединение разных источников (например, слияние таблиц).

Как трансформация реализуется в ETL и ELT?
Трансформация может происходить до или после загрузки данных в хранилище.

ETL (Extract – Transform – Load)
🔹 Трансформация происходит до загрузки в хранилище.
🔹 Данные приводятся к финальному виду до хранения.
🔹 Используется для строго структурированных данных и традиционных аналитических решений.

✅ Плюсы ETL:
Оптимизированные и чистые данные.
Быстрые аналитические запросы, так как данные уже подготовлены.

❌ Минусы ETL:
Длительная обработка перед загрузкой.
Сложно адаптировать к новым требованиям.

Примеры использования ETL:
✔️ Корпоративные хранилища данных (Data Warehouses).
✔️ Финансовая отчетность, где важны точность и стандарты.

ELT (Extract – Load – Transform)
🔹 Данные загружаются в исходном виде, а трансформация выполняется при запросах.
🔹 Используется в облачных хранилищах и аналитике больших данных.

✅ Плюсы ELT:
 Гибкость – можно анализировать данные по-разному.
 Масштабируемость – хорошо подходит для Big Data.

❌ Минусы ELT:
 Более сложные и ресурсоемкие запросы.
 Может потребовать дополнительных вычислительных мощностей.

Примеры использования ELT:
✔️ Аналитика в облаке (BigQuery, Snowflake, AWS Redshift).
✔️ Машинное обучение, где нужны сырые данные.

Ключевые моменты:
📌 Трансформация данных – это ключевой процесс подготовки данных к анализу.
📌 ETL выполняет преобразование до загрузки, обеспечивая структурированность данных.
📌 ELT загружает сырые данные, а трансформация выполняется по мере необходимости.
📌 Выбор метода зависит от потребностей бизнеса, объема данных и доступных технологий.
Загрузка данных в ETL и ELT
В традиционных ETL-процессах (Extract, Transform, Load) загрузка выполняется после предварительной обработки данных. Данные сначала извлекаются, затем трансформируются (очищаются, агрегируются, нормализуются), а только после этого загружаются в хранилище.

В ELT-процессах (Extract, Load, Transform) данные загружаются сразу после извлечения, в сыром виде, а трансформация происходит уже внутри хранилища или аналитической платформы.

Загрузка данных может выполняться разными методами в зависимости от требований бизнеса, архитектуры хранилища и объемов данных.

Основные стратегии загрузки данных
📌 Полная загрузка (Full Load)
Вся имеющаяся информация загружается заново.
 Используется при первичной настройке хранилища или обновлении данных.
Может занимать много времени и ресурсов.

📌 Инкрементальная загрузка (Incremental Load)
 Загружаются только новые или измененные данные.
 Оптимизирует ресурсы и снижает нагрузку на систему.
 В ETL требует механизма отслеживания изменений (например, временные метки, флаги обновления).

Методы загрузки данных
1. Пакетная загрузка (Batch Loading)
🔹 Данные загружаются периодически (раз в час, день, неделю).
🔹 Хорошо подходит для отчетности, финансовых данных, складских операций.
🔹 Применяется в традиционных ETL-решениях.

2. Потоковая загрузка (Streaming Loading)
🔹 Данные загружаются в реальном времени по мере поступления.
🔹 Используется в обработке событий (социальные сети, сенсоры IoT, биржевые данные).
🔹 Более характерна для ELT-решений, так как данные могут сразу записываться в хранилище.

Push и Pull: два подхода к загрузке данных
📌 Push-метод
Источник сам отправляет данные в хранилище или базу данных.
Подходит для потоковой обработки (например, IoT-устройства, лог-файлы).

📌 Pull-метод
 Хранилище само запрашивает данные у источника по расписанию.
 Применяется в пакетной обработке (например, обновление каталога товаров).

Серийная и параллельная загрузка
📌 Серийная загрузка (Serial Loading)
Данные загружаются по одному набору за раз.
Подходит для небольших объемов данных.

📌 Параллельная загрузка (Parallel Loading)
 Данные загружаются одновременно из нескольких источников или по разным каналам.
 Ускоряет обработку больших объемов данных.
Используется в Big Data-решениях и облачных системах.

ETL vs ELT: особенности загрузки
Метод	ETL	ELT
Очистка данных	До загрузки в хранилище	После загрузки в хранилище
Гибкость	Фиксированные процессы	Гибкая обработка в SQL/BI
Поддержка Big Data	Ограничена ресурсами	Использует облачные решения
Скорость загрузки	Медленнее, требует ETL-сервер	Быстрее, загружается сразу
Ключевые моменты:
📌 В ETL загрузка выполняется после трансформации, а в ELT – до нее.
📌 Основные стратегии загрузки – полная и инкрементальная.
📌 Пакетная обработка применяется для регулярных обновлений, потоковая – для данных в реальном времени.
📌 Push-метод отправляет данные автоматически, Pull-метод забирает данные по запросу.
📌 Параллельная загрузка ускоряет обработку больших объемов данных.
Что такое Apache Airflow?
Apache Airflow — это платформа для управления рабочими процессами (workflow orchestration).
✔Позволяет создавать, планировать и отслеживать выполнение DAG’ов — процессов, состоящих из последовательных или параллельных задач.
✔ Используется в ETL-пайплайнах, автоматизации процессов и обработке больших данных.

Ключевая идея: Airflow управляет зависимостями между задачами, гарантируя их правильное выполнение.

❗ Важно! Apache Airflow не обрабатывает данные, а организует выполнение задач, взаимодействующих с данными.

Основные компоненты Apache Airflow
Apache Airflow состоит из нескольких ключевых частей:

1️⃣ DAG (Directed Acyclic Graph) — Основная единица в Airflow
✔ DAG — это граф зависимостей задач, который управляет их порядком выполнения.
✔Каждая задача (task) выполняется по расписанию или при выполнении условий.

2️⃣ Планировщик (Scheduler)
✔ Отвечает за управление DAG'ами и их запуск в нужное время.
✔Определяет, какие задачи могут быть запущены на основе зависимостей.

3️⃣ Исполнитель (Executor)
✔Назначает задачи рабочим узлам (workers), которые их выполняют.
✔Может использовать локальные или распределенные ресурсы (например, Kubernetes, Celery).

4️⃣ Веб-интерфейс (Web Server)
✔ Графический интерфейс для мониторинга и управления DAG'ами.
📌 Позволяет:
✔ Запускать и останавливать DAG'и вручную.
✔ Смотреть логи выполнения задач.
✔ Проверять зависимые задачи.

5️⃣ Метаданные (Metadata Database)
✔ Хранит состояния DAG'ов и задач (успех, ошибка, повторный запуск).
✔ Используется Scheduler'ом и Web Server'ом для работы.

 Основные принципы работы Apache Airflow
Apache Airflow основан на 4 ключевых принципах:

✅ Масштабируемость (Scalability)
 Использует распределенную архитектуру, позволяя масштабировать выполнение DAG’ов.

✅ Динамичность (Dynamic Workflows)
DAG'и определяются в Python, что позволяет гибко управлять их логикой.

✅ Расширяемость (Extensibility)
Можно создавать кастомные операторы и плагины, интегрируя Airflow с любыми сервисами.

✅ Гибкость (Flexibility)
 DAG'и могут включать условные операторы, повторные попытки выполнения и таймауты.

Типичные сценарии использования Apache Airflow
Apache Airflow применяется в различных сферах:

ETL и обработка данных
✔ Управление загрузкой, трансформацией и выгрузкой данных.
✔ Автоматизация обновления аналитических отчетов.

Машинное обучение (ML Pipelines)
✔ Запуск обучения моделей.
✔ Управление предобработкой данных.

DevOps и автоматизация
✔ Управление CI/CD пайплайнами.
✔ Автоматическое развертывание инфраструктуры.

Big Data и Cloud-проекты
✔ Оркестрация процессов в облачных средах (AWS, GCP, Azure).
✔ Интеграция с Hadoop, Spark, Snowflake и другими платформами.

Ключевые моменты:
📌 Apache Airflow — это мощная платформа для управления рабочими процессами.
📌 Позволяет создавать DAG'и, управлять задачами и их зависимостями.
📌 Включает Scheduler, Executor, Web Server и Metadata Database.
📌 Поддерживает масштабируемость, динамическое создание пайплайнов и интеграции с другими сервисами.
Apache Airflow — это мощный инструмент для управления рабочими процессами обработки данных. Одним из ключевых преимуществ Airflow является представление процессов в виде направленного ациклического графа (DAG).

DAG позволяет:

✔ Структурировать процессы обработки данных.

✔ Управлять зависимостями между задачами.

✔ Повысить прозрачность выполнения процессов.

Что такое DAG?
DAG (Directed Acyclic Graph) — это граф, в котором задачи (узлы) соединены направленными связями (рёбрами), ацикличность которого означает отсутствие циклов. Это гарантирует, что ни одна задача не зависит от своей собственной завершённости.

DAG в Apache Airflow
В Apache Airflow DAG используется для организации последовательности выполнения задач обработки данных. Пример типового DAG-процесса:

1️⃣ Извлечение данных из источников (БД, API, файловых хранилищ).

2️⃣ Очистка и преобразование данных.

3️⃣ Загрузка очищенных данных в хранилище.

4️⃣ Создание аналитических отчетов и уведомление заинтересованных лиц.

Преимущества использования DAG в работе с данными
📌 Гибкость и масштабируемость
✔ DAG позволяет разбить сложные процессы обработки данных на небольшие задачи, которые могут выполняться параллельно.

✔ Это ускоряет обработку больших объемов данных и позволяет распределять нагрузку между рабочими узлами.

📌 Прозрачность выполнения процессов
✔ Графический интерфейс Apache Airflow позволяет мониторить выполнение задач.

✔ Удобный визуальный интерфейс помогает отслеживать ошибки и выявлять узкие места в обработке данных.

📌 Управление зависимостями
✔ В DAG можно четко задать порядок выполнения задач.

Например, в ETL-процессе:

✔ сначала выполняется извлечение данных,

✔ затем очистка и агрегация,

✔ после чего — анализ и визуализация.

✔ Если одна из задач завершается с ошибкой, последующие не запускаются.

📌 Автоматизация и оркестрация
✔ Airflow позволяет запускать DAG по расписанию.

✔ Поддерживает автоматический перезапуск упавших задач.

✔ Минимизирует участие человека в управлении процессами.

📌 Интеграция с различными источниками данных
✔ Airflow поддерживает работу с базами данных, облачными сервисами, API и файловыми системами.

✔ Это делает его универсальным инструментом для конвейеров обработки данных.

 Пример использования DAG в работе с данными в компании
📊 Сценарий:
Компания анализирует данные о продажах из нескольких источников: CRM-системы, базы данных магазинов и файловых хранилищ.

Задачи DAG:

✔ Собрать данные из разных систем.

✔ Очистить и нормализовать (удалить дубликаты, проверить корректность данных).

✔ Объединить и агрегировать данные для дальнейшего анализа.

✔ Загрузить очищенные данные в корпоративное хранилище (DWH).

✔ Сгенерировать отчеты и отправить уведомления аналитикам и руководству.

🔍 Как это выглядит в DAG?
Этот процесс можно представить в виде DAG, где каждая задача выполняется последовательно или параллельно:

[Извлечение данных] → [Очистка данных] → [Объединение данных] → [Загрузка в DWH] → [Отчеты и уведомления]
Если одна из задач завершается с ошибкой, Airflow позволяет автоматически перезапустить задачу или отправить уведомление о сбое.

Использование DAG в Apache Airflow значительно упрощает управление процессами обработки данных. DAG делает работу с данными прозрачной, гибкой и автоматизированной, что особенно важно для масштабируемых бизнес-процессов.

Ключевые моменты:
📌 DAG помогает структурировать и автоматизировать обработку данных.

📌 Обеспечивает мониторинг выполнения задач в реальном времени.

📌 Улучшает управление зависимостями и обработку ошибок.

📌 Позволяет масштабировать конвейеры обработки данных.

📌 Интегрируется с различными системами и источниками данных.

Apache Airflow и DAG — мощные инструменты, которые помогают компаниям эффективно управлять данными и автоматизировать аналитические процессы.
Apache Airflow позволяет управлять процессами обработки данных, строя их в виде направленных ациклических графов (DAG – Directed Acyclic Graph).
DAG (направленный ациклический граф) – это структура, в которой задачи выполняются в определённом порядке, исключая циклы. Это значит, что задачи в Airflow не могут зависеть друг от друга таким образом, чтобы образовывался замкнутый круг.

Пример DAG :
Допустим, вам нужно ежедневно обновлять отчёт о продажах. Процесс включает:

✔ Получение данных из базы.

✔ Очистку и обработку данных.

✔ Объединение данных с другими источниками.

✔ Загрузку обработанных данных в хранилище.

✔ Отправку отчёта заинтересованным лицам.

Этот процесс можно представить в виде DAG, где каждая операция – это отдельная задача, а зависимости между ними определяют порядок выполнения.

📌 Основные компоненты DAG
DAG в Airflow состоит из нескольких ключевых элементов:

1️⃣ Расписание (Schedule)
Вы можете настроить DAG так, чтобы он выполнялся по расписанию. Например:

✔  Каждые 10 минут

✔  Раз в день

✔  В определённое время

Настроить расписание можно с помощью стандартных выражений crontab или специальных параметров Airflow.

2️⃣ Аргументы DAG (DAG Arguments)
DAG может содержать глобальные настройки, которые применяются ко всем его задачам, например:

✔  Владелец DAG

✔ Дата начала выполнения

✔  Число повторных попыток при сбое

✔  Время ожидания перед повторной попыткой

Эти параметры помогают управлять поведением DAG.

3️⃣ Операторы (Operators) и Задачи (Tasks)
Операторы в Airflow – это строительные блоки DAG, которые определяют, какие именно задачи будут выполняться.
Они бывают нескольких типов:

✔  Bash-оператор – выполняет команды в терминале.

✔  Python-оператор – выполняет код на Python.

✔  SQL-оператор – выполняет SQL-запросы в базе данных.

✔  Sensor-оператор – ожидает наступления определённого события перед продолжением работы.

Каждая задача в DAG основана на каком-либо операторе. Например, задача может запускать скрипт Python, выполнять SQL-запрос или загружать данные.

4️⃣ Зависимости между задачами (Task Dependencies)
После создания задач их нужно связать друг с другом, определяя последовательность выполнения.
Примеры зависимостей:

✔  Линейная последовательность: Задачи выполняются одна за другой.

✔  Разветвлённый процесс: Одновременно выполняются несколько задач.

✔  Конвергентный процесс: Несколько задач должны завершиться, прежде чем запустится следующая.

Эти зависимости помогают структурировать рабочие процессы и управлять порядком выполнения задач.
📌 Процесс создания DAG в Airflow
Чтобы создать DAG, нужно выполнить несколько шагов:

✅ Шаг 1: Определение цели DAG
Перед созданием DAG важно понять, что именно он должен делать. Например:

✔ Обрабатывать данные каждый день в 7 утра

✔ Собирать информацию из API и записывать её в базу

✔ Обучать модель машинного обучения по расписанию

Чётко определённая цель поможет  структурировать задачи.

✅ Шаг 2: Определение задач
Декомпозируйте процесс на отдельные шаги. Например, если DAG отвечает за обработку данных, он может включать задачи:

✔ Извлечение данных из источника

✔ Очистка данных

✔ Запись данных в хранилище

✔ Формирование отчёта

Каждая из этих задач будет отдельным узлом DAG.

✅ Шаг 3: Установка зависимостей между задачами
Свяжите задачи так, чтобы они выполнялись в нужном порядке. Например:

✔ Последовательное выполнение: Очистка данных начинается только после их извлечения.

✔ Параллельное выполнение: Можно одновременно загружать данные из нескольких источников.

✔ Ожидание завершения других задач: Генерация отчёта начинается только после завершения обработки всех данных.

✅ Шаг 4: Определение расписания выполнения
Выберите, как часто DAG должен выполняться:

✔ Раз в сутки

✔ Каждый час

✔ При наступлении определённого события

Это позволяет автоматизировать процесс без вмешательства пользователя.

✅ Шаг 5: Тестирование DAG
Прежде чем запускать DAG в продакшен, его нужно протестировать:

✔ Проверить, правильно ли выполняются задачи.

✔ Убедиться, что все зависимости настроены корректно.

✔ Оценить время выполнения DAG.

 📌 Управление DAG через веб-интерфейс Airflow
После создания DAG его можно контролировать через веб-интерфейс Apache Airflow:

✔  Запуск DAG вручную – можно запустить DAG без ожидания расписания.
✔  Просмотр графика выполнения – визуализация задач и их зависимостей.
✔  Анализ логов – просмотр подробной информации о выполнении задач.
✔  Управление статусом DAG – можно приостановить DAG, если он временно не нужен.

Ключевые моменты:
📌 Apache Airflow позволяет автоматизировать выполнение сложных процессов.
📌 DAG – это граф задач, выполняемых в заданном порядке.
📌 Для создания DAG нужно определить задачи, их зависимости и расписание выполнения.
📌 Airflow UI помогает контролировать и управлять DAG в реальном времени.
 